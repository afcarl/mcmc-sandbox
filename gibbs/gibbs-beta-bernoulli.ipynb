{
 "metadata": {
  "name": "",
  "signature": "sha256:9c01f11ef0083f088d2d753e7b88af26056f765be20973663934c5a4c8100a62"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Gibbs sampling for the beta-bernoulli mixture model"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bayesian beta-bernoulli mixture model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a fixed $K$ and $D$, the beta-bernoulli mixture model is a generative model describing $D$-dimensional binary vectors ($y_i \\in \\{0,1\\}^{D}$) drawn from a $K$-component mixture.\n",
      "\n",
      "We can describe the probabilistic model as:\n",
      "\\begin{align*}\n",
      "  \\pi|\\alpha &\\sim \\text{Dirichlet}(\\{ \\frac{\\alpha}{K} \\}_{i=1}^{K}) \\\\\n",
      "  c_i|\\pi &\\sim \\text{Discrete}(\\pi) \\\\\n",
      "  p_d^{k} | \\beta,\\gamma &\\sim \\text{Beta}(\\beta, \\gamma) \\\\\n",
      "  y_i^{d} | c_i{=}k,p_d^{k} &\\sim \\text{Bernoulli}(p_d^{k})\n",
      "\\end{align*}\n",
      "Let us clear up some of this notation. $\\pi$ is a $K$-dimensional vector living in the $(K-1)$-dimensional probability simplex. For each $k \\in [K]$, $\\{p_d^k\\}_{d=1}^{D} \\in [0,1]^{D}$. Given $N$ data points, the assignment vector $\\{c_i\\}_{i=1}^{N} \\in \\{0, ..., K-1\\}^{N}$. The hyperparameters of this model are $\\mathcal{H} = (\\alpha, \\beta, \\gamma)$.\n",
      "\n",
      "As we will see later on, this model has nice analytical properties for Gibbs sampling, since the beta distribution is a nice conjugate prior for the bernoulli distribution.\n",
      "\n",
      "The inference problem we will consider is the following. Given a dataset $\\mathcal{Y} = \\{y_i\\}_{i=1}^{N}$, we want to learn the posterior distribution on the assignment vector $\\mathcal{C} = \\{c_i\\}_{i=1}^{N}$. That is, we want to be able to estimate and draw samples from $p(\\mathcal{C} | \\mathcal{Y}; \\mathcal{H})$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Gibbs sampling for estimating the posterior"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This problem can be solved efficiently with [Gibbs sampling](http://en.wikipedia.org/wiki/Gibbs_sampling), which is another [Markov-chain monte carlo](http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) (MCMC) variant. Let the notation $\\mathcal{C}_{\\neg i} = \\{ c_j \\in \\mathcal{C} : j \\neq i \\}$. The Gibbs sampler assumes we can efficiently sample from the distribution $p(c_i | \\mathcal{C}_{\\neg i}, \\mathcal{Y}; \\mathcal{H})$. Every iteration of the sampling then works by\n",
      "sampling for each $i \\in [N]$,\n",
      "\\begin{align*}\n",
      "    c^{(t)}_{i} \\gets p(c_i | \\{ c_j \\in \\mathcal{C}^{(t)} : j < i \\}, \\{ c_j \\in \\mathcal{C}^{(t-1)} : j > i \\}, \\mathcal{Y}; \\mathcal{H})\n",
      "\\end{align*}\n",
      "\n",
      "Let the notation $\\mathcal{Y}^{k} = \\{ y_i \\in \\mathcal{Y} : c_i = k \\}$ and $\\mathcal{Y}^{k}_{\\neg i} = \\{ y_j \\in \\mathcal{Y} : c_j = k, j \\neq i \\}$.\n",
      "It turns out, for the beta-bernoulli model, we can indeed [derive](http://homepage.tudelft.nl/19j49/Publications_files/TR_1.pdf) that\n",
      "\\begin{align*}\n",
      "  p(c_i{=}k|\\mathcal{C}_{\\neg i}, \\mathcal{Y};\\mathcal{H}) \\propto\n",
      "    \\frac{ |\\mathcal{Y}^{k}_{\\neg i}| + \\frac{\\alpha}{K} }{ N - 1 + \\alpha } \\prod_{d=1}^{D} \\frac{(\\beta+\\sum_{y_k\\in \\mathcal{Y}^{k}} y_k^{d})^{y_i^d} (  \\gamma + |\\mathcal{Y}^{k}| - \\sum_{y_k\\in \\mathcal{Y}^{k}} y_k^{d})^{(1-y_i^{d})} }{ \\beta + \\gamma + |\\mathcal{Y}^{k}| }\n",
      "\\end{align*}\n",
      "We can construct $p(c_i{=}k|\\mathcal{C}_{\\neg i}, \\mathcal{Y};\\mathcal{H})$ exactly by then enumerating through all $K$ clusters and normalizing."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Implementation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as sp\n",
      "import scipy.misc\n",
      "\n",
      "def discrete_sample(pmf):\n",
      "    # XXX: does numpy have something to do this?\n",
      "    coin = np.random.random()\n",
      "    cdf = np.cumsum(pmf)\n",
      "    a = np.where(coin >= cdf)[0]\n",
      "    if not a.shape[0]:\n",
      "        return 0\n",
      "    return min(a[-1]+1, pmf.shape[0]-1)\n",
      "\n",
      "def gibbs_beta_bernoulli(Y, K, alpha, beta, gamma, niters):\n",
      "    N, D = Y.shape\n",
      "    alpha, beta, gamma = map(float, [alpha, beta, gamma])\n",
      "\n",
      "    # start with random assignment\n",
      "    assignments = np.random.randint(0, K, size=N)\n",
      "\n",
      "    # initialize the sufficient statistics (cluster sums) accordingly\n",
      "    sums = np.zeros((K, D)) \n",
      "    cnts = np.zeros(K)\n",
      "    for yi, ci in zip(Y, assignments):\n",
      "        sums[ci] += yi\n",
      "        cnts[ci] += 1\n",
      "\n",
      "    history = np.zeros((niters, N)) \n",
      "    for t in xrange(niters):\n",
      "        for i, (yi, ci) in enumerate(zip(Y, assignments)):\n",
      "            # remove from SS\n",
      "            sums[ci] -= yi\n",
      "            cnts[ci] -= 1\n",
      "\n",
      "            # build log P(c_i=k | c_{\\i}, Y)\n",
      "            def fn(k):\n",
      "                lg_term1 = np.log(cnts[k] + alpha/K)\n",
      "                lg_term2 = np.log(N - 1 + alpha)\n",
      "                lg_term3 = D*np.log(beta + gamma + cnts[k] + 1) \n",
      "                def fn1(tup):\n",
      "                    d, yid = tup \n",
      "                    if yid:\n",
      "                        return np.log(beta + sums[k, d] + 1)\n",
      "                    else:\n",
      "                        return np.log(gamma + cnts[k] - sums[k, d]) \n",
      "                lg_term4 = sum(map(fn1, enumerate(yi)))\n",
      "                return lg_term1 - lg_term2 - lg_term3 + lg_term4\n",
      "\n",
      "            lg_dist = np.array(map(fn, xrange(K)))\n",
      "            lg_dist -= sp.misc.logsumexp(lg_dist) # normalize\n",
      "            dist = np.exp(lg_dist)\n",
      "\n",
      "            # reassign\n",
      "            ci = discrete_sample(dist)\n",
      "            assignments[i] = ci\n",
      "            sums[ci] += yi\n",
      "            cnts[ci] += 1\n",
      "        history[t] = assignments\n",
      "\n",
      "    return history"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us first generate a small toy dataset (using the beta-bernoulli model as the generative process). Then we will test our Gibbs sampler on this small dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha, beta, gamma = 1., 1., 2.\n",
      "K = 2\n",
      "D = 3\n",
      "N = 5\n",
      "\n",
      "pis = np.random.dirichlet(alpha/K*np.ones(K))\n",
      "cis = np.array([discrete_sample(pis) for _ in xrange(N)])\n",
      "aks = np.random.beta(beta, gamma, size=(K, D))\n",
      "\n",
      "def bernoulli(p):\n",
      "    return 1. if np.random.random() <= p else 0.\n",
      "\n",
      "Y = np.zeros((N, D))\n",
      "for i in xrange(N):\n",
      "    Y[i] = np.array([bernoulli(aks[cis[i], d]) for d in xrange(D)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "niters = 50000\n",
      "chain = gibbs_beta_bernoulli(Y, K, alpha, beta, gamma, niters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the natural question is, how do we verify that the Gibbs sampler is indeed drawing samples from $p(\\mathcal{C} | \\mathcal{Y})$. This is a bit trickier in the Bayesian case than the non-Bayesian case, since what we are really after is a *distribution* instead of, e.g. a point estimate.\n",
      "\n",
      "Luckily, for small problems (small $K$ and $N$), we can actually calculate $p(\\mathcal{C} | \\mathcal{Y}; \\mathcal{H})$ exactly by brute force enumeration. This is because $p(\\mathcal{C} | \\mathcal{Y} ; \\mathcal{H})$ is a discrete distribution of size $K^{N}$, and we can analytically calculate the joint distribution $p(\\mathcal{C}, \\mathcal{Y}; \\mathcal{H})$; from the joint, the posterior follows by $p(\\mathcal{C} | \\mathcal{Y} ) = \\frac{p(\\mathcal{C}, \\mathcal{Y})}{\\sum_{c} p(c ,\\mathcal{Y})}$, where the summation in the denominator is over all possible $K^N$ assignment vectors (from here on, we drop the hyperparameter dependence to ease notation).\n",
      "\n",
      "For an arbitrary $\\mathcal{C}, \\mathcal{Y}$, we have $p(\\mathcal{C}, \\mathcal{Y}) = p(\\mathcal{C}) p(\\mathcal{Y} | \\mathcal{C})$. From [page 2](http://homepage.tudelft.nl/19j49/Publications_files/TR_1.pdf), we have \n",
      "\\begin{align*}\n",
      "  p(\\mathcal{C}) = \\frac{\\Gamma(\\alpha)}{\\Gamma(|\\mathcal{Y}|+\\alpha)}\n",
      "      \\prod_{k=1}^{K} \\frac{\\Gamma( |\\mathcal{Y}^{k}| + \\frac{\\alpha}{K})}{ \\Gamma(\\frac{\\alpha}{K})}\n",
      "\\end{align*}\n",
      "where $\\Gamma(\\cdot)$ is the [Gamma](http://en.wikipedia.org/wiki/Gamma_function) function.\n",
      "We can derive $p(\\mathcal{Y}|\\mathcal{C})$ as follows.\n",
      "\\begin{align*}\n",
      "  p(\\mathcal{Y} | \\mathcal{C}) &= \n",
      "    \\prod_{k=1}^{K} p(\\mathcal{Y}^{k} | \\mathcal{C}^{k}) \\\\\n",
      "    &= \\prod_{k=1}^{K} \\int_{\\Theta_k} [\\prod_{y_i \\in \\mathcal{Y}^k} p(y_i | \\Theta_k) ] p(\\Theta_k ; \\mathcal{H}) \\; d\\Theta_k \n",
      "\\end{align*}\n",
      "Now for each $k$, we evaluate the inner integral as:\n",
      "\\begin{align*}\n",
      "  \\int_{\\theta_1,...,\\theta_D} \\prod_{d=1}^{D} \\left(\\prod_{y_i \\in \\mathcal{Y}^k} \\theta_d^{y_i^d} (1-\\theta_d)^{1-y_i^d}\\right) \\frac{1}{B(\\beta,\\gamma)} \\theta_d^{\\beta-1}(1-\\theta_d)^{\\gamma-1} \\; d\\theta_1...\\theta_{D}\n",
      "\\end{align*}\n",
      "where $B(\\beta,\\gamma)$ is the [Beta function](http://en.wikipedia.org/wiki/Beta_function). Noting that $\\int_{0}^{1} x^{(m-1)} (1-x)^{(n-1)} \\; dx = \\frac{\\Gamma(m)\\Gamma(n)}{\\Gamma(m+n)}$, we can simplify the above integral to:\n",
      "\\begin{align*}\n",
      "\\frac{1}{B(\\beta,\\gamma)^{D}} \\prod_{d=1}^{D} \\frac{\\Gamma( \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\beta) \\Gamma( |\\mathcal{Y}^{k}| - \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\gamma)}{ \\Gamma( |\\mathcal{Y}^{k}| + \\beta + \\gamma )}\n",
      "\\end{align*}\n",
      "And therefore,\n",
      "\\begin{align*}\n",
      "  p(\\mathcal{Y} | \\mathcal{C}) = \\frac{1}{B(\\beta,\\gamma)^{KD}} \\prod_{k=1}^{K} \\prod_{d=1}^{D} \\frac{\\Gamma( \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\beta) \\Gamma( |\\mathcal{Y}^{k}| - \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\gamma)}{ \\Gamma( |\\mathcal{Y}^{k}| + \\beta + \\gamma )}\n",
      "\\end{align*}\n",
      "\n",
      "Therefore, we can compute the posterior distribution of the data as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy as sp\n",
      "import scipy.special\n",
      "import itertools as it\n",
      "import math\n",
      "\n",
      "def lg_pr_joint(C, Y, K, alpha, beta, gamma):\n",
      "    N, D = Y.shape\n",
      "    nks = np.bincount(C, minlength=K)\n",
      "    assert nks.shape[0] == K\n",
      "    assert C.shape[0] == N\n",
      "\n",
      "    # log P(C)\n",
      "    gammaln = sp.special.gammaln\n",
      "    betaln = sp.special.betaln\n",
      "    term1 = gammaln(alpha) - gammaln(N + alpha) - K*gammaln(alpha/K)\n",
      "    term2 = sum(gammaln(nk + alpha/K) for nk in nks)\n",
      "    lg_pC = term1 + term2\n",
      "\n",
      "    # log P(Y|C)\n",
      "    term1 = K*D*betaln(beta, gamma)\n",
      "    term2 = D*sum(gammaln(nk + beta + gamma) for nk in nks)\n",
      "    sums = np.zeros((K, D))\n",
      "    for yi, ci in zip(Y, C):\n",
      "        sums[ci] += yi\n",
      "    def fn1(nk, sum_yid):\n",
      "        assert nk >= sum_yid\n",
      "        return gammaln(sum_yid + beta) + gammaln(nk - sum_yid + gamma)\n",
      "    term3 = sum(sum(fn1(nk, yid) for yid in row) for nk, row in zip(nks, sums))\n",
      "    lg_pYgC = -term1 - term2 + term3\n",
      "    \n",
      "    return lg_pC + lg_pYgC\n",
      "\n",
      "def brute_force_posterior(Y, K, alpha, beta, gamma):\n",
      "    N, _ = Y.shape\n",
      "\n",
      "    # enumerate K^N cluster assignments\n",
      "    lg_pis = np.array([lg_pr_joint(np.array(C), Y, K, alpha, beta, gamma) for C in it.product(range(K), repeat=N)])\n",
      "    lg_pis -= sp.misc.logsumexp(lg_pis)\n",
      "    \n",
      "    return np.exp(lg_pis)\n",
      "\n",
      "actual_posterior = brute_force_posterior(Y, K, alpha, beta, gamma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To measure the distance between the actual posterior distribution and that produced by our Gibbs sampler, we compare the KL-divergence of the actual and the empirical. Recall for discrete distributions the KL-divergence (relative entropy) is defined as\n",
      "\\begin{align*}\n",
      "  D(P||Q) = \\sum_{x} P(x) \\log\\frac{P(x)}{Q(x)}\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "smoothing = 1e-5\n",
      "skip = 100\n",
      "\n",
      "def kl(a, b):\n",
      "    return np.sum([p*np.log(p/q) for p, q in zip(a, b)])\n",
      "\n",
      "def histify(history, K):\n",
      "    _, N = history.shape\n",
      "    # generate an ID for each K^N element\n",
      "    idmap = { C : i for i, C in enumerate(it.product(range(K), repeat=N)) }\n",
      "    hist = np.zeros(K**N, dtype=np.float)\n",
      "    for h in history:\n",
      "        hist[idmap[tuple(h)]] += 1.0\n",
      "    return hist\n",
      "\n",
      "def fn(i):\n",
      "    hist = histify(chain[:i:skip], K) + smoothing\n",
      "    hist /= hist.sum()\n",
      "    return kl(actual_posterior, hist)\n",
      "\n",
      "iters = range(0, niters, skip)[1:]\n",
      "kls = map(fn, iters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pylab as plt\n",
      "\n",
      "plt.plot(iters, kls)\n",
      "plt.xlabel('Iterations')\n",
      "plt.ylabel('KL-divergence')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "<matplotlib.text.Text at 0x10a6481d0>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEPCAYAAABcA4N7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGbdJREFUeJzt3XuYXHWd5/F3dedKOiRAJFyS0CEGFRbkMs6gEC3kIo44\neHkWYXGEMIzjKhJcZQF3R9tZ9wFncdBlBh1XYcXhMsrKRScwio+NCiRcDOESQQmBhISEECA3aJLu\n1P7xPWVVV1dXKumqPqe73q/nOU/VOVXd51eHUJ/+Xc7vB5IkSZIkSZIkSZIkSZIkSZLqcC2wDnis\n7NjewM+B3wM/A6amUC5J0i5oa+Lvvg44teLYpURQHAL8ItmXJLWwTvrXKJ4EpifP90v2JUkZ1swa\nRTXTieYoksfpNd4rScqA4Q6KcoVkkyRl2JhhPt86oslpLbA/8GK1N82ZM6ewfPny4SyXJI0Gy4E3\nN/qXDneN4g7gnOT5OcBt1d60fPlyCoWCW6HAl7/85dTLkJXNa+G18FrU3oA5zfjibmZQ3ATcB7wF\nWAXMB64ATiaGx7432ZckZVgzm57OGuT4SU08pySpwdLszFYd8vl82kXIDK9FideixGvRfLm0CzCI\nQtLeJkmqUy6XgyZ8r1ujkCTVZFBIkmoyKCRJNRkUkqSaDApJUk0GhSSpJoNCklSTQSFJqsmgkCTV\nZFBIkmoyKCRJNWU2KJYtg9/8Ju1SSJIyOyng9OkF1q0D5waUpPo0a1LAzAZFcTltg0KS6tOSs8ce\neGDaJZAkZTooZsxIuwSSpEwHhTUKSUpfpoNiwoS0SyBJynRQ7NiRdgkkSZkOir6+tEsgSTIoJEk1\nGRSSpJoyGxTjxxsUkpQFmQ2KKVPszJakLMh0UFijkKT0ZTYopk41KCQpCwwKSVJNBoUkqSaDQpJU\nU2aDwlFPkpQNmQ4KaxSSlL7MBsWhhxoUkpQFmQ2Kzk6DQpKyILNB0d5uUEhSFqQVFJcBTwCPATcC\n4yvf0N5uZ7YkZUEaQdEJ/DVwNHA40A6cWfmmtjZrFJKUBWNSOOcmYDuwB9CXPK6ufJNNT5KUDWnU\nKF4Gvg6sBNYArwJ3V77JoJCkbEijRjEHuIhogtoI/Ag4G7ih/E1XX93Fhg3Q1QX5fJ58Pj/MxZSk\nbOvu7qa7u7vp58k1/QwDfQw4GTg/2f9L4FjgM2XvKTz3XIHjjoNVq4a7eJI0MuVyOWjC93oaTU9P\nEsEwkfhAJwHLKt/kqCdJyoY0gmIpcD3wEPBocuw7lW9y1JMkZUMaTU/1KKxbV+Cww2D9+rSLIkkj\nw2hqeqqLo54kKRsMCklSTZkOCjuzJSl9mQ0KO7MlKRsyGxQ2PUlSNhgUkqSaDApJUk2ZDYq2pGR2\naEtSujIbFODIJ0nKgkwHhSOfJCl9mQ4K+ykkKX0GhSSpJoNCklSTQSFJqinTQdHW5qgnSUpbpoPC\nGoUkpc+gkCTVZFBIkmoyKCRJNWU+KOzMlqR0ZToonMJDktKX6aCw6UmS0mdQSJJqMigkSTUZFJKk\nmjIdFE7hIUnpy3RQWKOQpPQZFJKkmgwKSVJN9QTFJOBvgf+T7M8FTmtaicoYFJKUvnqC4jpgG/Cu\nZH8N8D+bVqIyTuEhSemrJyjmAF8jwgJga/OK059TeEhS+uoJijeAiWX7c5JjTWfTkySlb0wd7+kC\n7gJmADcCxwHnNq9IJQaFJKWvnqD4GfBb4Nhk/0LgpaaVqIxBIUnpq6fp6SNAL/DTZOsFPjTE804F\nbgF+ByyjFEL9GBSSlL56guLLwKtl+68SzVFD8U1gIfA24AgiMAYWzik8JCl19TQ95aocax/COacA\n84Bzkv1eYGO1N1qjkKT01VOjeBj4B2K005uBq5Jju2s2sJ64P+O3xI18e1R7o0EhSemrJyg+C2wH\n/hW4GegBPjOEc44BjgauSR63ApdWe6NBIUnpq6fpaQtwSQPP+XyyPZjs30KVoOjq6uLxx2HrVpgx\nI08+n29gESRp5Ovu7qa7u7vp56nW/1DpLcAXgE5KwVIA3juE8/4KOB/4PdExPpH+YVQoFArMnw/v\nfjfMnz+EM0lSi8jlclDf9/ouqadG8SPgW8B3gWJDUGGI5/0scAMwDlgOVI0Cp/CQpPTVExTbiaBo\npKXAO3b2JvsoJCl99XRm/4TovN4f2LtsazqDQpLSV0+N4lyiqekLFcdnN7w0FQwKSUpfPUHR2exC\nDMagkKT0ZX6FO6fwkKR0ZXqFO0c9SVL6Mr3CnU1PkpQ+V7iTJNXkCneSpJoyv8KdndmSlK56guIY\n4j6KNcQcIrOINSWeI9aSaJq2Nti+vZlnkCTtTD1B8U9EWDya7B8OPEGExX8G/r05RYsaRU9Ps367\nJKke9XRmrwGOJMLimOT5M8DJwN83r2j2UUhSFtQTFG8hahBFy4C3ErO+DnUW2ZoMCklKXz1NT08Q\ns8feTPRRnEGExXhiZtmmMSgkKX311CjOIWoPFwELiGanc4iQGMriRTvlqCdJSt/OahRjgIXACcCV\nVV7f3PASlXEKD0lK385qFL3ADmDqMJRlAJueJCl99fRRbAUeA35OaZ6nAnHjXVMZFJKUvnqC4sfJ\nVhzhlKPJo52KDApJSl89QfF/gT2IO7KfbGppKtiZLUnpq2fU018AS4iJAQGOAu5oWonK2JktSemr\nJyi6gD8DXkn2lwAHN6tA5Wx6kqT01RMU24FXK44NS4OQQSFJ6asnKJ4Azib6M+YCVwP3NbNQRQaF\nJKWvnqD4LHAYsardTcAm4i7tpjMoJCl99Yx6egvwxWQbVo56kqT01VOj+AdiWOz/AP5Dc4vTn6Oe\nJCl99QRFnpjr6SXgn4m7tP+2iWX6I5ueJCl99QQFwAvAN4FPAUuBLzWtRGUMCklKXz1BcShxL8Xj\nwD8SI54ObGKZ/sigkKT01dOZfS2xaNH7gNXNLU5/dmZLUvrqCYpjm16KQdiZLUnpqxUUPwL+I9F5\nXakAHNGUEpWx6UmS0lcrKBYkjx8cjoJUY1BIUvpqBcWa5PHZYShHVQaFJKWvVlBsYfAFigrAno0v\nTn8GhSSlr1ZQdCSPXyVqF/+S7J8NHNCAc7cDDwHPM0jzlqOeJCl99S5cdA0xGeAm4FvA6Q049wJg\nGTWWVXXUkySlr56g2Ap8nKgBtBM1ii1DPO8M4M+B7xJrcFdVq+nptdeGWAJJUl3qCYr/BJwBrEu2\nM5JjQ3EVcDE7WQBpsKD4yU9g+nTDQpKGQz1BsYJofpqWbKcztJFQpwEvEkuqDlqbgOpB8dxzcP75\nMG0afPWrsGnTEEoiSdqpeu7MLvdb4OghnvNdRPD8OTCBGD11PfCJ8jd1dXXx8svw0kvQ3Z0nn8/z\nxhtwxhlw8cVx/PLLYe5cmD9/iCWSpBGou7ub7u7upp+n5l/0VSwBjmrg+d8DfIGBo54KhUKBFSvg\nhBPg2Wdj9NPZZ0NvL/zwh7B1K3z603DwwdDV1cASSdIIlcvlYNe/13eq3mnGi/6t0QWgxqin8qan\n738fnnkGrr8ecjno6IATT4Q//KEJJZIk/dGuBsV/Tx5XNuj89xDNUFUVg6JQgNtvhwsvhIkTS6/P\nnQtPP92gkkiSqtrVoChqeNWmmvZ26OmBo46KoDj55P6vH3IIPPWUo58kqZl2NyiGxcSJ8MorcMAB\ncOutsO++/V+fNg1OPRWuvDKd8klSK6g16unzNV7rqPFaw0yZEqObpk6N2kU1H/84XHPNcJRGklpT\nraCYzOAdzd9oQlmq2mef2q/PmgUrq/SYvPACvPxy1DqmT29O2SSpFdQKimsZvNM6tTUqKpUHxbJl\ncU/FiSfCd74TxzZsiH6O8ePTK6MkjWS1+ih+Dsyucvw84JvNKc6umzIlRkXdcQfk83DEEXHfxQMP\nwOrVMGNGPEqSdk+tGsXngJ8BHwB+nxy7jJgU8N1NLlfdcrmoLZx+OixcCO9/f//XZ8+OGsfBB6dT\nPkka6WoFxULgDeBOYn6n84E/BeYBrzS/aPWbMQM+9KGBIQHRNLVq1fCXSZJGi53N9fQLYD5xY9y9\nwHuBnmYXalctWRI1i2oG6+yWJNWn3qVQJwAnAuuT/WFZCrVeg4UERFAsWTJ8ZZGk0aZWZ3YHMUR2\nMjAWmFS2n5mQ2JmZM61RSNJQZPrO7Eaw6UmShqZlgqIw6By1kqRadnXhohFnyhRoa4ONG2MqEIB1\n62IiwSVLYOxYOO202v0cktTKRn2NAkq1irVrY6ryQw6JKcqvvhrOOw/uvjvtEkpSdrVEUBx0EFx1\nVUxX/tprcef21q3wy1/C174Gp5wC554LixalXVJJyp6sNrgUCg3sVHjiCfj2t2O97XnzKk8E69fH\n8alTYfHihp1WkoZVs5ZCbYmgqMd998HnPw/33z+sp5WkhsnKmtmj1uTJsHlz2qWQpOwxKBIGhSRV\nZ1AkDApJqs6gSHR0RFB4Y54k9WdQJMaPj5vutm1LuySSlC0GRRmbnyRpIIOijEEhSQMZFGUMCkka\nyKAo09EBW7akXQpJyhaDoow1CkkayKAoY1BI0kAGRZk5c+BLX4q1KyRJwUkBy+zYAWeeCXvvDW99\nKxx/fDzv6IB9943X24xWSRnl7LHD5NFHoasrwuHBB6N20dMDEyfCK6/ArbfC+96XStEkqSaDIkVr\n18Lrr8N118Xd21/5StolkqSBmhUUo37N7EbYb794PPBAeOihdMsiScPNFvddcMABsGZN2qWQpOFl\nUOwCg0JSK0ojKGYCvwSeAB4HLkyhDLvFoJDUitLozN4v2R4BOoCHgQ8Bvyt7T6Y6s4v6+mDChBgN\n1dcXQ2U3bYJ77oHFi6G9He64I+1SSmpVo3nU023A1cAvyo5lMigAPvlJuPfeCIUdOyI4TjgBZs+G\nz3wm5oqaNCntUkpqRaM1KDqBe4DDgPLp+DIbFLUce2zcqPeBD8Cb3hTNVBs2RHisWBEjpsaNg85O\nuOGGCBtJapTRODy2A7gFWED/kACgq6vrj8/z+Tz5fH64yrXbPvUpuP12uPlmWL8e9t8fpk2LGsbh\nh8Nf/VXUQi65BI45Jn7moovg3HNTLbakEaq7u5vu7u6mnyetGsVY4KfAncA3qrw+ImsU9dq4Me4A\nX7QomrFuuy3tEkkaDUZT01MO+D6wAfjcIO8Z1UFRtGJFzCe1enXaJZE0GjQrKNIYHnsc8HHgBGBJ\nsp2aQjlS19kZ/Rj5PPT2pl0aSaoujaD4TXLeI4Gjku2uFMqRulwO7rsvHo8/Hn7wg7RLJEkDpT3q\naTAt0fRUtGkTLFwYHdtr16ZdGkkj1Wjqo6hHSwUFxA18e+wRoTF+fNqlkTQSjaY+ClXR3h5ThDz/\nfNolkaT+DIoMmTkTVq1KuxSS1J9BkSGzZsHKlWmXQpL6MygyZNYsaxSSssegyJCZM61RSMoegyJD\nrFFIyiKDIkOsUUjKIoMiQ+zMlpRFBkWGTJkChULMLitJWeGd2RlzzDGwzz6xzGpRby+cfz6ceWZ6\n5ZKUfU7h0SLWroVf/QomTy4de/jhWLvipz9Nr1ySss+gaGGrVsGf/EmESC6r/8Ukpc6gaGGFAuy3\nH/zN38CECbEG9557wjveASeemHbpJGWFkwK2sFwOrroKtm+HzZuhoyPW5P7oR52WXFLzWaMYwc49\nNwLj1GR9wDfegJ6eqHFs3gxbt8Jrr8Vj8XlPD3zve1EbkTS62PSkAdasgZtugqefjmnKJ0yItSwm\nTYrO8I6OWONi0qTSdu21MH06fOUraZdeUqMZFGqIhQvhyivhzjujBvL667FY0muvweGH9x+WK2lk\nMSjUEK++CjNmwLZtpRrIlCnRJHXeefB3f5d2CSXtLoNCDVMoDBxmu3IlvP3t8OyzERySRh6DQk33\nkY9EJ/dJJ0UNA2Du3Hjcd9+ohfT0RJNVseO8+HzHjqih9PSUOs6POw6mTk3v80itxqBQ0/3613DB\nBTBuXHzpb9sGzzwTr730UjRTFbdis1XxMZeLkJgwITrNn3sOFiyAiy5K9zNJraRZQTGm0b9QI9e8\nebB0afXXqjVX1XLFFTEqS9LI5xgX1WVXpw7Zf3944YXmlEXS8DIo1BQGhTR6GBRqCoNCGj0MCjWF\nQSGNHo56UlMUCjEC6vLLYexY6OsbfOvtjbvCzzor7VJLI5ujnjSiFGe8feqpCIP29sG3ceNiKG1P\nDxx9dNz4Jyk7rFEoE265BW64Ie7lWLMmwkPSrvGGO7WEefPgsMNi3fAtW6KWsWNHNGUVtbXFlsuV\nnhe33t5Yt2Pbtniste21Fxx5JMycGT9X/NnyxwkTSotD9fb2by6rfNxrryh38ffv2DH4Vijs2vFc\nDsaMiefFu+F7e+NYe3uUr3iNBnucODHm+SoOda58rHas+LjXXjETcfHzFgqDb/vsA7NmNebfg3aN\nQaGW8OCD8OMfx93dHR3xRV0MhVyu9GU02Bdre3v0iYwbF4+1tvXrYcmS6HQvHhszpv/j2rWweHHp\nC7n4WP68+Lh+fczEW/z59vaBQTZYwO3seKEQX9LFprrx4+McxZAa7Au+/HHjRli3LvYrQ6TasfLX\n1q+PcCp+ruJ/j2rbM8/ENm1a4/99qDaDQtKI8LGPxRxhJ51Uu7ZU3C/+UTDYHwG7emzSpFjMqxWn\nzDcoJI0IixbBpZeWahjVakzF57lcTCK5ZUt9763n9YceiunyP/jB0kCK4hxloz08RltQnAp8A2gH\nvgt8reJ1g0LSbunujvXk+/oiGPr6Sv06uVypT6dYI5k8OfpUZsyIZr0LLoBTTkn1I+y2ZgVFGvna\nDvwjERaHAmcBb0uhHCNCd3d32kXIDK9FideipPJa5POwYUMs0vXyy9E3UxwUsX17aRr8bdvi2MqV\ncOONcOGF8M53ukxwNWncR/GnwNPAs8n+zcDpwO9SKEvmdXd3k8/n0y5GJngtSrwWJfVei2Jtolij\nKJo6NbYjjoh+la9/HT784RjpNXbsrpWlrW3goIfKrdqAhba2/oMIaj0f7LXi2jHNkEZQHAisKtt/\nHvizFMohSf2MGwf33w+PPRa1kb6++n+22JleOYS6chaCyqHPfX2lIdDlo9RqPa/22vbtjbkG1aQR\nFHY+SMqsuXOb+9d5M33iE835vWl0Zh8LdBF9FACXATvo36H9NDBneIslSSPecuDNaReiEcYQH6YT\nGAc8gp3ZkqQK7weeImoOl6VcFkmSJEmjyanAk8AfgEtSLkujXAusAx4rO7Y38HPg98DPgKllr11G\nfP4ngfJbf45JfscfgG+WHR8P/GtyfBFwUGOL31AzgV8CTwCPAxcmx1vxekwAFhPNr8uAy5PjrXgt\nitqBJcBPkv1WvRbPAo8S1+KB5FirXosB2onmqE5gLKOn/2IecBT9g+Lvgf+aPL8EuCJ5fijxuccS\n1+FpSoMOHiDuQwFYSGlAwKeBa5LnHyPuTcmq/YAjk+cdRBPk22jd67FH8jiG+B/2eFr3WgD8F+AG\n4I5kv1WvxQoiGMq16rUY4J3AXWX7lybbaNBJ/6B4EpiePN8v2Yf4y6C8JnUXMVJsf/rflHgm8O2y\n9xTvRRkDrG9UoYfBbcBJeD32AB4EDqN1r8UM4G7gBEo1ila9FiuAfSqOpXYtsjZFVrWb8Q5MqSzN\nNp1ojiJ5LP4DOID43EXFa1B5fDWla1N+3XqBjQz8aySLOoma1mJa93q0EX8NrqPUJNeq1+Iq4GJi\nuHxRq16LAhGaDwF/nRxL7VpkbSnUVr0Zr0DrffYO4P8BC4DNFa+10vXYQTTFTQH+nfhrulyrXIvT\ngBeJNvn8IO9plWsBcBzwAvAmol/iyYrXh/VaZK1GsZro7CyaSf9EHE3WEdVHiCrii8nzymswg7gG\nq5PnlceLP1NcU2wM8aXzcuOL3DBjiZD4AdH0BK19PSD+ovs3ovOxFa/Fu4C/IJpcbgLeS/z7aMVr\nARESEE1CtxL9DK16LQYYzTfjdTKwM7vYrngpAzumxgGzietR7JhaTLQr5hjYMfWt5PmZZLtjKgdc\nTzQzlGvF6zGN0siVicCvgBNpzWtR7j2U+iha8VrsAUxOnk8C7iVGMrXitRjUaLwZ7yZgDbCNaBec\nT7QH3k31oW5fJD7/k8D7yo4Xh7o9DfzvsuPjgR9SGurW2YTP0CjHE80tjxDNDEuIf7yteD0OB35L\nXItHifZ5aM1rUe49lEY9teK1mE38m3iEGEJe/B5sxWshSZIkSZIkSZIkSZIkSZIkSdq5LcnjQcBZ\nDf7dX6zYv7fBv1+SNAyKc0flKd3hW6+dzX9WOS+VJGkEKn6ZLwJeJe7+XkDMbfa/iDn6lwKfTN6X\nB34N3E5p8rXbiJk7H6c0e+cVxEybS4h5iKBUe8klv/sx4i7rM8p+dzfwI2LK538pK+cVxGyxS5Of\nlSQNk2JQlM8ZBBEM/y15Pp5Y/6GT+DLfQv9VvvZKHicSX/7F/coaRXH/o8S0CjlgX+A5YgK3PBFW\nBySv3UfMDLoP/WcE3bPeDycNt6zNHis1Uq5i/xTgE0SNYBExd86bk9ceIL7cixYQc+3cT8zMOXcn\n5zoeuJGY+vlF4B7gHcn+A8RcX4Xkdx5EhEcP8D3gw8Dru/rhpOFiUKjVXEAslnQUMIeYZA1ga9l7\n8sQsrscSa0UsIda3rqXAwGAqrhfwRtmxPmKa9T5i6uhbiLUY7kLKKINCo9lmStM1QywM9GlKHdaH\nUFqzutyewCvEX/xvJQKjaDvVO7x/Taw93EYsNvNuoiZRGR5Fk4jZP+8k1ol++04/jZSSrK1wJzVC\n8S/5pcRf7o8A1xHTLHcSU3vniCaiDzNwtbC7gE8By4gp7+8ve+07RGf1w8Bflv3crcSa70uTYxcn\nv/9tDFyJrEAE2O1ETSUHfG63P60kSZIkSZIkSZIkSZIkSZIkSZIkSZKa7/8DvS0rCChPQRQAAAAA\nSUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10a622690>"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}