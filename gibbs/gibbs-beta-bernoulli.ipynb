{
 "metadata": {
  "name": "",
  "signature": "sha256:2e8747c5d29deadca433c40635ab3b7ebae67a4df1d21da0face8c4e03bab52a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Gibbs sampling for the beta-bernoulli mixture model"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bayesian beta-bernoulli (fixed size) mixture model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a fixed $K$ and $D$, the beta-bernoulli mixture model is a generative model describing $D$-dimensional binary vectors ($y_i \\in \\{0,1\\}^{D}$) drawn from a $K$-component mixture.\n",
      "\n",
      "We can describe the probabilistic model as:\n",
      "\\begin{align*}\n",
      "  \\pi|\\alpha &\\sim \\text{Dirichlet}(\\{ \\frac{\\alpha}{K} \\}_{k=1}^{K}) \\\\\n",
      "  c_i|\\pi &\\sim \\text{Discrete}(\\pi) \\\\\n",
      "  p_d^{k} | \\beta,\\gamma &\\sim \\text{Beta}(\\beta, \\gamma) \\\\\n",
      "  y_i^{d} | c_i{=}k,p_d^{k} &\\sim \\text{Bernoulli}(p_d^{k})\n",
      "\\end{align*}\n",
      "Let us clear up some of this notation. $\\pi$ is a $K$-dimensional vector living in the $(K-1)$-dimensional probability simplex. For each $k \\in [K]$, $\\{p_d^k\\}_{d=1}^{D} \\in [0,1]^{D}$. Given $N$ data points, the assignment vector $\\{c_i\\}_{i=1}^{N} \\in \\{0, ..., K-1\\}^{N}$. The hyperparameters of this model are $\\mathcal{H} = (\\alpha, \\beta, \\gamma)$.\n",
      "\n",
      "As we will see later on, this model has nice analytical properties for Gibbs sampling, since the beta distribution is a nice conjugate prior for the bernoulli distribution.\n",
      "\n",
      "The inference problem we will consider is the following. Given a dataset $\\mathcal{Y} = \\{y_i\\}_{i=1}^{N}$, we want to learn the posterior distribution on the assignment vector $\\mathcal{C} = \\{c_i\\}_{i=1}^{N}$. That is, we want to be able to estimate and draw samples from $p(\\mathcal{C} | \\mathcal{Y}; \\mathcal{H})$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Gibbs sampling for estimating the posterior"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This problem can be solved efficiently with [Gibbs sampling](http://en.wikipedia.org/wiki/Gibbs_sampling), which is another [Markov-chain monte carlo](http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) (MCMC) variant. Let the notation $\\mathcal{C}_{\\neg i} = \\{ c_j \\in \\mathcal{C} : j \\neq i \\}$. The Gibbs sampler assumes we can efficiently sample from the distribution $p(c_i | \\mathcal{C}_{\\neg i}, \\mathcal{Y}; \\mathcal{H})$. Every iteration of the sampling then works by\n",
      "sampling for each $i \\in [N]$,\n",
      "\\begin{align*}\n",
      "    c^{(t)}_{i} \\gets p(c_i | \\{ c_j \\in \\mathcal{C}^{(t)} : j < i \\}, \\{ c_j \\in \\mathcal{C}^{(t-1)} : j > i \\}, \\mathcal{Y}; \\mathcal{H})\n",
      "\\end{align*}\n",
      "\n",
      "Let the notation $\\mathcal{Y}^{k}_{\\neg i} = \\{ y_j \\in \\mathcal{Y} : c_j = k, j \\neq i \\}$.\n",
      "It turns out, for the beta-bernoulli model, we can indeed [derive](http://homepage.tudelft.nl/19j49/Publications_files/TR_1.pdf) that\n",
      "\\begin{align*}\n",
      "  p(c_i{=}k|\\mathcal{C}_{\\neg i}, \\mathcal{Y};\\mathcal{H}) \\propto\n",
      "    \\frac{ |\\mathcal{Y}^{k}_{\\neg i}| + \\frac{\\alpha}{K} }{ N - 1 + \\alpha } \\prod_{d=1}^{D} \\frac{(\\beta+\\sum_{y_k\\in \\mathcal{Y}^{k}_{\\neg i}} y_k^{d})^{y_i^d} (  \\gamma + |\\mathcal{Y}^{k}_{\\neg i}| - \\sum_{y_k\\in \\mathcal{Y}^{k}_{\\neg i}} y_k^{d})^{(1-y_i^{d})} }{ \\beta + \\gamma + |\\mathcal{Y}^{k}_{\\neg i}| }\n",
      "\\end{align*}\n",
      "We can construct $p(c_i{=}k|\\mathcal{C}_{\\neg i}, \\mathcal{Y};\\mathcal{H})$ exactly by then enumerating through all $K$ clusters and normalizing."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Implementation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll first implement a sampler from a discrete distribution. Note while `scipy.stats.rv_discrete` [(Docs)](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_discrete.html) implements this functionality, it is quite heavy weight to call within a loop, so we roll our own."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# workspace setup\n",
      "import numpy as np\n",
      "import scipy as sp\n",
      "import scipy.io\n",
      "import scipy.misc\n",
      "import scipy.special\n",
      "\n",
      "%matplotlib inline\n",
      "import matplotlib.pylab as plt\n",
      "\n",
      "import itertools as it\n",
      "import math"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def discrete_sample(pmf):\n",
      "    coin = np.random.random()\n",
      "    acc = 0.0\n",
      "    n, = pmf.shape\n",
      "    for i in xrange(n):\n",
      "        acc0 = acc + pmf[i]\n",
      "        if coin <= acc0:\n",
      "            return i\n",
      "        acc = acc0\n",
      "    return n-1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok enough playing around! Now the actual Gibbs sampler implementation. Note we make use of [numpy broadcasting](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) for performance:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gibbs_beta_bernoulli(Y, K, alpha, beta, gamma, niters):\n",
      "    N, D = Y.shape\n",
      "    alpha, beta, gamma = map(float, [alpha, beta, gamma])\n",
      "\n",
      "    # start with random assignment\n",
      "    assignments = np.random.randint(0, K, size=N)\n",
      "\n",
      "    # initialize the sufficient statistics (cluster sums) accordingly\n",
      "    sums = np.zeros((D, K), dtype=np.int64)\n",
      "    cnts = np.zeros(K, dtype=np.int64)\n",
      "    for yi, ci in zip(Y, assignments):\n",
      "        sums[:,ci] += yi\n",
      "        cnts[ci] += 1\n",
      "\n",
      "    history = np.zeros((niters, N), dtype=np.int64)\n",
      "\n",
      "    # precomputations\n",
      "    nplog = np.log\n",
      "    npexp = np.exp\n",
      "    nparray = np.array\n",
      "    logsumexp = sp.misc.logsumexp\n",
      "    lg_denom = nplog(N - 1 + alpha)\n",
      "    alpha_over_K = alpha/K\n",
      "    beta_plus_gamma = beta + gamma\n",
      "\n",
      "    for t in xrange(niters):\n",
      "        for i, (yi, ci) in enumerate(zip(Y, assignments)):\n",
      "            # remove from SS\n",
      "            sums[:,ci] -= yi\n",
      "            cnts[ci] -= 1\n",
      "\n",
      "            lg_term1 = nplog(cnts + alpha_over_K) - lg_denom - D*nplog(beta_plus_gamma + cnts)\n",
      "            lg_term2 = nplog(beta + sums)\n",
      "            lg_term3 = nplog(gamma + cnts - sums)\n",
      "\n",
      "            lg_dist = lg_term1 + (lg_term2*yi[:,np.newaxis] + lg_term3*(1-yi[:,np.newaxis])).sum(axis=0)\n",
      "            lg_dist -= logsumexp(lg_dist) # normalize\n",
      "\n",
      "            # reassign\n",
      "            ci = discrete_sample(npexp(lg_dist))\n",
      "            assignments[i] = ci\n",
      "            sums[:,ci] += yi\n",
      "            cnts[ci] += 1\n",
      "        history[t] = assignments\n",
      "\n",
      "    return history"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us first generate a small toy dataset (using the beta-bernoulli model as the generative process). Then we will test our Gibbs sampler on this small dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha, beta, gamma = 2., 1., 1.\n",
      "K = 2\n",
      "D = 3\n",
      "N = 4\n",
      "\n",
      "pis = np.random.dirichlet(alpha/K*np.ones(K))\n",
      "cis = np.array([discrete_sample(pis) for _ in xrange(N)])\n",
      "aks = np.random.beta(beta, gamma, size=(K, D))\n",
      "\n",
      "print 'Pi:', pis\n",
      "print 'C :', cis\n",
      "\n",
      "def bernoulli(p):\n",
      "    return 1 if np.random.random() <= p else 0\n",
      "\n",
      "Y = np.zeros((N, D), dtype=np.int64)\n",
      "for i in xrange(N):\n",
      "    Y[i] = np.array([bernoulli(aks[cis[i], d]) for d in xrange(D)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pi: [ 0.56867172  0.43132828]\n",
        "C : [0 0 0 1]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "niters = 50000\n",
      "chain = gibbs_beta_bernoulli(Y, K, alpha, beta, gamma, niters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Posterior distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the natural question is, how do we verify that the Gibbs sampler is indeed drawing samples from $p(\\mathcal{C} | \\mathcal{Y})$. This is a bit trickier in the Bayesian case than the non-Bayesian case, since what we are really after is a *distribution* instead of, e.g. a point estimate.\n",
      "\n",
      "Luckily, for small problems (small $K$ and $N$), we can actually calculate $p(\\mathcal{C} | \\mathcal{Y}; \\mathcal{H})$ exactly by brute force enumeration. This is because $p(\\mathcal{C} | \\mathcal{Y} ; \\mathcal{H})$ is a discrete distribution of size $K^{N}$, and we can analytically calculate the joint distribution $p(\\mathcal{C}, \\mathcal{Y}; \\mathcal{H})$; from the joint, the posterior follows by $p(\\mathcal{C} | \\mathcal{Y} ) = \\frac{p(\\mathcal{C}, \\mathcal{Y})}{\\sum_{c} p(c ,\\mathcal{Y})}$, where the summation in the denominator is over all possible $K^N$ assignment vectors (from here on, we drop the hyperparameter dependence to ease notation).\n",
      "\n",
      "For an arbitrary $\\mathcal{C}, \\mathcal{Y}$, we have $p(\\mathcal{C}, \\mathcal{Y}) = p(\\mathcal{C}) p(\\mathcal{Y} | \\mathcal{C})$. From [page 2](http://homepage.tudelft.nl/19j49/Publications_files/TR_1.pdf), we have \n",
      "\\begin{align*}\n",
      "  p(\\mathcal{C}) = \\frac{\\Gamma(\\alpha)}{\\Gamma(|\\mathcal{Y}|+\\alpha)}\n",
      "      \\prod_{k=1}^{K} \\frac{\\Gamma( |\\mathcal{Y}^{k}| + \\frac{\\alpha}{K})}{ \\Gamma(\\frac{\\alpha}{K})}\n",
      "\\end{align*}\n",
      "where $\\Gamma(\\cdot)$ is the [Gamma](http://en.wikipedia.org/wiki/Gamma_function) function.\n",
      "We can derive $p(\\mathcal{Y}|\\mathcal{C})$ as follows.\n",
      "\\begin{align*}\n",
      "  p(\\mathcal{Y} | \\mathcal{C}) &= \n",
      "    \\prod_{k=1}^{K} p(\\mathcal{Y}^{k} | \\mathcal{C}^{k}) \\\\\n",
      "    &= \\prod_{k=1}^{K} \\int_{\\Theta_k} [\\prod_{y_i \\in \\mathcal{Y}^k} p(y_i | \\Theta_k) ] p(\\Theta_k ; \\mathcal{H}) \\; d\\Theta_k \n",
      "\\end{align*}\n",
      "Now for each $k$, we evaluate the inner integral as:\n",
      "\\begin{align*}\n",
      "  \\int_{\\theta_1,...,\\theta_D} \\prod_{d=1}^{D} \\left(\\prod_{y_i \\in \\mathcal{Y}^k} \\theta_d^{y_i^d} (1-\\theta_d)^{1-y_i^d}\\right) \\frac{1}{B(\\beta,\\gamma)} \\theta_d^{\\beta-1}(1-\\theta_d)^{\\gamma-1} \\; d\\theta_1...\\theta_{D}\n",
      "\\end{align*}\n",
      "where $B(\\beta,\\gamma)$ is the [Beta function](http://en.wikipedia.org/wiki/Beta_function). Noting that $\\int_{0}^{1} x^{(m-1)} (1-x)^{(n-1)} \\; dx = \\frac{\\Gamma(m)\\Gamma(n)}{\\Gamma(m+n)}$, we can simplify the above integral to:\n",
      "\\begin{align*}\n",
      "\\frac{1}{B(\\beta,\\gamma)^{D}} \\prod_{d=1}^{D} \\frac{\\Gamma( \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\beta) \\Gamma( |\\mathcal{Y}^{k}| - \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\gamma)}{ \\Gamma( |\\mathcal{Y}^{k}| + \\beta + \\gamma )}\n",
      "\\end{align*}\n",
      "And therefore,\n",
      "\\begin{align*}\n",
      "  p(\\mathcal{Y} | \\mathcal{C}) = \\frac{1}{B(\\beta,\\gamma)^{KD}} \\prod_{k=1}^{K} \\prod_{d=1}^{D} \\frac{\\Gamma( \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\beta) \\Gamma( |\\mathcal{Y}^{k}| - \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\gamma)}{ \\Gamma( |\\mathcal{Y}^{k}| + \\beta + \\gamma )}\n",
      "\\end{align*}\n",
      "\n",
      "Therefore, we can compute the posterior distribution of the data as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def all_assignment_vectors():\n",
      "    return it.product(range(K), repeat=N)\n",
      "\n",
      "def lg_pr_joint(C, Y, K, alpha, beta, gamma):\n",
      "    N, D = Y.shape\n",
      "    nks = np.bincount(C, minlength=K)\n",
      "    assert nks.shape[0] == K\n",
      "    assert C.shape[0] == N\n",
      "\n",
      "    # log P(C)\n",
      "    gammaln = sp.special.gammaln\n",
      "    betaln = sp.special.betaln\n",
      "    term1 = gammaln(alpha) - gammaln(N + alpha) - K*gammaln(alpha/K)\n",
      "    term2 = sum(gammaln(nk + alpha/K) for nk in nks)\n",
      "    lg_pC = term1 + term2\n",
      "\n",
      "    # log P(Y|C)\n",
      "    term1 = K*D*betaln(beta, gamma)\n",
      "    term2 = D*sum(gammaln(nk + beta + gamma) for nk in nks)\n",
      "    sums = np.zeros((K, D))\n",
      "    for yi, ci in zip(Y, C):\n",
      "        sums[ci] += yi\n",
      "    def fn1(nk, sum_yid):\n",
      "        assert nk >= sum_yid\n",
      "        return gammaln(sum_yid + beta) + gammaln(nk - sum_yid + gamma)\n",
      "    term3 = sum(sum(fn1(nk, yid) for yid in row) for nk, row in zip(nks, sums))\n",
      "    lg_pYgC = -term1 - term2 + term3\n",
      "    \n",
      "    return lg_pC + lg_pYgC\n",
      "\n",
      "def brute_force_posterior(Y, K, alpha, beta, gamma):\n",
      "    N, _ = Y.shape\n",
      "\n",
      "    # enumerate K^N cluster assignments\n",
      "    lg_pis = np.array([lg_pr_joint(np.array(C), Y, K, alpha, beta, gamma) for C in all_assignment_vectors()])\n",
      "    lg_pis -= sp.misc.logsumexp(lg_pis)\n",
      "    \n",
      "    return np.exp(lg_pis)\n",
      "\n",
      "# generate an ID for each K^N element\n",
      "idmap = { C : i for i, C in enumerate(all_assignment_vectors()) }\n",
      "revidmap = { i : C for i, C in enumerate(all_assignment_vectors()) }\n",
      "\n",
      "actual_posterior = brute_force_posterior(Y, K, alpha, beta, gamma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's pause for a second and take a look at the [MAP](http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) estimator:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'P(C=actual|Y):', actual_posterior[idmap[tuple(cis)]]\n",
      "print 'max_C P(C|Y):', actual_posterior.max()\n",
      "print 'argmax_C P(C|Y):', revidmap[actual_posterior.argmax()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P(C=actual|Y): 0.0326701428774\n",
        "max_C P(C|Y): 0.200725357839\n",
        "argmax_C P(C|Y): (0, 0, 0, 0)\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice how the ground truth cluster assignment is *not* the MAP estimator!\n",
      "\n",
      "To measure the distance between the actual posterior distribution and that produced by our Gibbs sampler, we compare the KL-divergence of the actual and the empirical. Recall for discrete distributions the KL-divergence (relative entropy) is defined as\n",
      "\\begin{align*}\n",
      "  D(P||Q) = \\sum_{x} P(x) \\log\\frac{P(x)}{Q(x)}\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "smoothing = 1e-5\n",
      "skip = 100\n",
      "skipped_chain = chain[::skip]\n",
      "window = 10000\n",
      "\n",
      "def kl(a, b):\n",
      "    return np.sum([p*np.log(p/q) for p, q in zip(a, b)])\n",
      "\n",
      "def histify(history, K):\n",
      "    _, N = history.shape\n",
      "    hist = np.zeros(K**N, dtype=np.float)\n",
      "    for h in history:\n",
      "        hist[idmap[tuple(h)]] += 1.0\n",
      "    return hist\n",
      "\n",
      "def fn(i):\n",
      "    hist = histify(skipped_chain[max(0, i-window):(i+1)], K) + smoothing\n",
      "    hist /= hist.sum()\n",
      "    return kl(actual_posterior, hist)\n",
      "\n",
      "kls = map(fn, xrange(skipped_chain.shape[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's take a peek at the posterior distribution of the *last* window compared with our brute force posterior distribution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "H = histify(skipped_chain[skipped_chain.shape[0]-1-window:], K) + smoothing\n",
      "H /= H.sum()\n",
      "print \"C\\t\\tP_g(C|Y)\\tP_a(C|Y)\\t|Diff|\"\n",
      "for c, (a, b) in zip(all_assignment_vectors(), zip(H, actual_posterior)):\n",
      "    def f(x): return '%.7f' % (x)\n",
      "    print \"\\t\".join(map(str, [c,f(a),f(b),f(math.fabs(a-b))]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C\t\tP_g(C|Y)\tP_a(C|Y)\t|Diff|\n",
        "(0, 0, 0, 0)\t0.2060000\t0.2007254\t0.0052746\n",
        "(0, 0, 0, 1)\t0.0360000\t0.0326701\t0.0033299\n",
        "(0, 0, 1, 0)\t0.0960000\t0.0980104\t0.0020104\n",
        "(0, 0, 1, 1)\t0.0820000\t0.0688359\t0.0131641\n",
        "(0, 1, 0, 0)\t0.0220000\t0.0326701\t0.0106701\n",
        "(0, 1, 0, 1)\t0.0060000\t0.0172090\t0.0112089\n",
        "(0, 1, 1, 0)\t0.0200000\t0.0172090\t0.0027910\n",
        "(0, 1, 1, 1)\t0.0340000\t0.0326701\t0.0013299\n",
        "(1, 0, 0, 0)\t0.0320000\t0.0326701\t0.0006701\n",
        "(1, 0, 0, 1)\t0.0140000\t0.0172090\t0.0032089\n",
        "(1, 0, 1, 0)\t0.0160000\t0.0172090\t0.0012089\n",
        "(1, 0, 1, 1)\t0.0260000\t0.0326701\t0.0066701\n",
        "(1, 1, 0, 0)\t0.0640000\t0.0688359\t0.0048359\n",
        "(1, 1, 0, 1)\t0.1040000\t0.0980104\t0.0059896\n",
        "(1, 1, 1, 0)\t0.0200000\t0.0326701\t0.0126701\n",
        "(1, 1, 1, 1)\t0.2219999\t0.2007254\t0.0212746\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This looks reasonable. We can also get a sense for the trend as we increase the number of iterations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(np.arange(0, chain.shape[0], skip) + 1, kls)\n",
      "plt.xlabel('Iterations')\n",
      "plt.ylabel('Posterior KL-divergence')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "<matplotlib.text.Text at 0x7fe2d2459a50>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEPCAYAAABCyrPIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fe2d2631c50>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Posterior predictive distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another distribution we can look at is the posterior *predictive* distribution $p(y|\\mathcal{C},\\mathcal{Y};\\mathcal{H})$, that is, the distribution over the *next* data point. To derive this, we first note that $p(y|\\mathcal{C},\\mathcal{Y}) = \\sum_{k=1}^{K} p(y,c{=}k|\\mathcal{C},\\mathcal{Y}) = \\sum_{k=1}^{K} p(c{=}k|\\mathcal{C}) p(y|\\mathcal{Y}_{k})$.\n",
      "\n",
      "From [Eq. 9, pg. 3](http://homepage.tudelft.nl/19j49/Publications_files/TR_1.pdf), we have $p(c{=}k|\\mathcal{C}) = \\frac{|\\mathcal{C}_{k}| + \\frac{\\alpha}{K}}{|\\mathcal{C}| + \\alpha}$. Next, $p(y|\\mathcal{Y}_{k})$ is simply the posterior predictive of the beta-bernoulli likelihood model. That is, $p(y_d{=}1|\\mathcal{Y}_{k})= \\frac{\\beta + \\sum_{y_i \\in \\mathcal{Y}_{k}} y_i^d}{ \\beta + \\gamma + |\\mathcal{Y}_{k}|  } $. Putting it together, we have\n",
      "\\begin{align*}\n",
      "  p(y|\\mathcal{C},\\mathcal{Y}) = \\sum_{k=1}^{K} \\frac{|\\mathcal{C}_{k}| + \\frac{\\alpha}{K}}{|\\mathcal{C}| + \\alpha} \\prod_{d=1}^{D}  (\\hat{p}_d^k)^{y_d} (1-\\hat{p}_d^k)^{1-y_d}\n",
      "\\end{align*}\n",
      "where $\\hat{p}_d^k = \\frac{\\beta + \\sum_{y_i \\in \\mathcal{Y}_{k}} y_i^d}{ \\beta + \\gamma + |\\mathcal{Y}_{k}| } $.\n",
      "\n",
      "We can also look at the non-Bayesian case, where we know the exact values for $\\pi$ and $\\{ p_d^k \\}$.\n",
      "\\begin{align*}\n",
      "  p(y | \\pi, \\{p_d^k\\}) = \\sum_{k=1}^{K} \\pi_k \\prod_{d=1}^{D} (p_d^k)^{y_d} (1-p_d^k)^{1-y_d}\n",
      "\\end{align*}\n",
      "\n",
      "This makes explicit how our prior information affects our estimates of $\\hat{\\pi}$ and $\\hat{p_d^k}$. Now for the reference distributions, we know both the ground truth cluster assignment and the exact model parameters, so we can compute $ p(y|\\mathcal{C},\\mathcal{Y}) $ and $ p(y | \\pi, \\{p_d^k\\})$ exactly. For our Gibbs sampler, we estimate $p(y|\\mathcal{C},\\mathcal{Y})$ by averaging over our samples of $\\mathcal{C}$. Details below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def posterior_predictive(C, Y, K, alpha, beta, gamma):\n",
      "    N, D = Y.shape\n",
      "    assert C.shape[0] == N\n",
      "\n",
      "    nks = np.bincount(C, minlength=K)\n",
      "    sums = np.zeros((K, D), dtype=np.int64) \n",
      "    for yi, ci in zip(Y, C): \n",
      "        sums[ci] += yi\n",
      "\n",
      "    def fn(yvalue):\n",
      "        def fn1(nk, sum_yid, yd):\n",
      "            assert nk >= sum_yid\n",
      "            theta = (beta + sum_yid) / (beta + gamma + nk) \n",
      "            assert theta >= 0.0 and theta <= 1.0 \n",
      "            return np.log(theta) if yd else np.log(1.-theta)\n",
      "        def fn2(nk, row):\n",
      "            assert len(yvalue) == row.shape[0]\n",
      "            term1 = np.log(nk + alpha/K) - np.log(N + alpha)\n",
      "            term2 = sum(fn1(nk, sum_yid, yd) for sum_yid, yd in zip(row, yvalue))\n",
      "            return term1 + term2\n",
      "        return sp.misc.logsumexp([fn2(nk, row) for nk, row in zip(nks, sums)])\n",
      "\n",
      "    yvalues = it.product([0, 1], repeat=D)\n",
      "    lg_pr_yvalue = map(fn, yvalues)\n",
      "    return np.exp(lg_pr_yvalue)\n",
      "\n",
      "def posterior_predictive_nonbayes(pis, aks):\n",
      "    K, = pis.shape\n",
      "    assert aks.shape[0] == K\n",
      "    _, D = aks.shape\n",
      "    \n",
      "    def fn(yvalue):\n",
      "        def fn2(yd, theta_kd):\n",
      "            return np.log(theta_kd) if yd else np.log(1.-theta_kd)\n",
      "        def fn1(pi_k, theta_k):\n",
      "            return np.log(pi_k) + sum(fn2(yd, theta_kd) for yd, theta_kd in zip(yvalue, theta_k))\n",
      "        return sp.misc.logsumexp([fn1(pi_k, theta_k) for pi_k, theta_k in zip(pis, aks)])\n",
      "    \n",
      "    yvalues = it.product([0, 1], repeat=D)\n",
      "    lg_pr_yvalue = map(fn, yvalues)\n",
      "    return np.exp(lg_pr_yvalue)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reference distributions\n",
      "actual_posterior_predictive = posterior_predictive(cis, Y, K, alpha, beta, gamma)\n",
      "nonbayes_posterior_predictive = posterior_predictive_nonbayes(pis, aks)\n",
      "\n",
      "posterior_predictives = np.array([\n",
      "    posterior_predictive(assignment, Y, K, alpha, beta, gamma) for assignment in skipped_chain])\n",
      "    \n",
      "def fn1(i):\n",
      "    posteriors = posterior_predictives[min(0, i-window):(i+1)].mean(axis=0)\n",
      "    return kl(actual_posterior_predictive, posteriors)\n",
      "\n",
      "posterior_predictive_kls = map(fn1, xrange(posterior_predictives.shape[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PH = posterior_predictives[posterior_predictives.shape[0]-1-window:].mean(axis=0)\n",
      "print \"y\\t\\tP(y|C) [gibbs]\\tP(y|C) [actual]\\tP(y|params)\\t|gibbs-actual|\"\n",
      "for y, ((a, b), c) in zip(it.product([0,1],repeat=D), zip(zip(PH, actual_posterior_predictive), nonbayes_posterior_predictive)):\n",
      "    print \"\\t\".join(map(str, [y, a, b, c, math.fabs(a-b)]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "y\t\tP(y|C) [gibbs]\tP(y|C) [actual]\tP(y|params)\t|gibbs-actual|\n",
        "(0, 0, 0)\t0.0696879583333\t0.0813827160494\t0.0702718653765\t0.011694757716\n",
        "(0, 0, 1)\t0.074270837963\t0.0726913580247\t0.0311105038123\t0.00157947993827\n",
        "(0, 1, 0)\t0.0490761157407\t0.046024691358\t0.0156461945261\t0.00305142438272\n",
        "(0, 1, 1)\t0.0430206435185\t0.0443456790123\t0.0268061808693\t0.00132503549383\n",
        "(1, 0, 0)\t0.236138986111\t0.226765432099\t0.337324350195\t0.00937355401235\n",
        "(1, 0, 1)\t0.254263328704\t0.241382716049\t0.198344794157\t0.0128806126543\n",
        "(1, 1, 0)\t0.143519162037\t0.134716049383\t0.111252060647\t0.00880311265432\n",
        "(1, 1, 1)\t0.130022967593\t0.152691358025\t0.209244050416\t0.0226683904321\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's take a look at the posterior predictive distributions of the *last* window compared to the ground truth distributions:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice how our Gibbs estimate of the distribution does not quite match the non-Bayesian one, which makes sense. Since $N$ is small in our case, our prior distribution is still weighing in non-negligibly. Finally, let's take a look at the KL-divergence (w.r.t. the *Bayesian* distribution with ground truth clusters) as the number of iterations increases:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(np.arange(0, niters, skip) + 1, posterior_predictive_kls)\n",
      "plt.xlabel('Iterations')\n",
      "plt.ylabel('Posterior predictive KL-divergence')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "<matplotlib.text.Text at 0x7fe2d2346090>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEPCAYAAAB7rQKTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiIiIiIiIiIpnr/wOPC6ge2iAXTgAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fe2d2631910>"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that this distribution seems to converge more slowly than the posterior in the previous section. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "usps_digits = sp.io.loadmat('data/usps_resampled/usps_resampled.mat')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "usps_class_2 = usps_digits['train_labels'][2,:] == 1\n",
      "usps_Y0 = usps_digits['train_patterns'][:,usps_class_2]\n",
      "usps_Y = np.zeros(usps_Y0.shape)\n",
      "usps_Y[usps_Y0>0.1] = 1.0\n",
      "plt.imshow(usps_Y[:,3].reshape((16,16)), cmap=plt.cm.binary)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "<matplotlib.image.AxesImage at 0x7fe2d0df7dd0>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAPwAAAD7CAYAAABOrvnfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnW2MLNlZ3389/VZVPTsz966FDXiVJZaQSJQ4kAQRB7Sb\n2JYcxzGKEiVBJDHeJF8CiQnEeA2K2f0QyQsiJi/iA0mMbIEdgokckAjBlrIkUYgBYxsHMGDz5rXl\nBfnembm3q/p98qHmqTl9uqqn+3RVdVfV85NKXf0yU9Wnz7/OOc85/3pAURRFURRFURRFURRFURRF\nURRFUZSq8sQTT1wBuummW8nbtfZSaWW9kQNXV1fpx33mmWd45plnCjy0Hm8TwjDk3r173Lt3jy9+\n8YvJvr3Z70VR5HSeQRBw9+5d7t69y6OPPprs25v93nPPPVeJ8jyUY7VaLcjQ9tEO//d1wKeA3wbe\ntsP/URSlJFwF3wb+HbHo/wTwTcBX5XVSiqIUg6vgvxb4NPB7wBT4T8A3bvrHTz75pONh3dDjVZs6\nl2fZ381V8F8OfNZ4/sL1axtR5x+wCccrmzqXZ9nfreP4d5lRQGU3JNB5dXW1tH/bey6Mx2MmkwnT\n6ZTZbMZisVj6f61Wi3a7TafTodvt0uv16Pf7zsfs9/v0ej263S6dToejoyMJMCXfZT6fM5vNmE6n\nTCYTxuMxrVYr+Zy9bz/arynLuAr+c8BjxvPHiFv5Jczo45NPPln7lmhXrq6uWCwWyWY/z3rPVYCj\n0YjLy0uGwyGj0YjxeMxsNuPq6oqjoyM6nQ69Xg/f95lOp8zncyC+ULjgeR4nJycMBgN836ff79Pp\ndGi1WiwWC2azGZPJhCiK6Ha7tNvt5O+Ojo6WtlartfKa/XpTRP/888/z/PPPb/RZ1xLpAL8JvBr4\nPPCLxIG73zA+kzktp6SzWCyYz+fJNpvNlp5nvb5YLJyON5lMePjwYbINh8Ol52mvDYdDJpOJ0/F6\nvR7Hx8fJNhgMlp6nvTYYDPA8j3a7nfQ2ZN/c0l4/OtplEqq6rJuWc23hZ8C3Af+dOGL/H1kWu+KI\niF662Zs8ugp+Op0SRVGymS18q9VaauEBjo6O6Ha7zGYzp+N1u1183082s4W/urpaauGlLKbTafI5\nGQrIo7m/WCzodrvAzVBEWcVV8AD/7XpTckK66dKKTyaTZJMxbdpr0tXelvl8nozj5dHu0vf7feBG\n7J7nOR+v3W7T7/eTsXyv11vp0stwQcQ+Go2Sz8om8YRer8d8Pqfb7SbDGunSa+8ynV0Er+SMCN4O\nWo3H42SMnba5trhyLHtbLBZJCw/LLfsuQwi5iNibCFS+h4hduukSLOz3+3iel+zb5yNib7fbKvgM\nVPAHhNnCm4KPoojRaLTyKNt0OnU+phkAlOi/2cK32+2kBZXP7YIZWJOoutnCy3eX182ehe/7TCYT\nfN9PPmu27DKWV7Fno4I/MOwuvbTuYRgShiFRFK3su7bwwJKwTAFKC5/23i6YFw7zAiMtfNp7MvY3\npxBF7HJOcmGSFl9Fn44K/oDI6tKbgpeouewPh0PnFl66v9K1tvez3nMVvXkxE9Ga+1nvdbtdxuNx\nMjVot+zSG5Gx/a69kDqjgj8g0rr00oUPwzCZGnvw4MHSVJnrNJk5PpZHuOl2S9DO/Eyv13OOgM/n\n86UAoRmgkxbeDiJOJhPa7XYSULTH7ObCoH6/ry38LajgD4x1XXpp3R88eMDl5WXy6Cp4c2wcBAFw\nE6Czp+XMTYJ52yJRd7mAyfedTqcr03Lm1m63N2rZpQeggs9GBX9ArAvaRVGUCP7y8pKLi4vk0XXl\nW6/XYzAYJFF5Ebs9Lef7/tKiGJnv3pbJZMJwOEyi8iL2tJV25qKfdrudrCo0x+xyQTKj9jLLoKSj\ngj8g0sbwZos4HA6TVv3i4oLz83POz88ZjUZOxzPn1E2Bm9NyvV6PIAg4Pj7m5OSEk5OTpOu/LaPR\nKBkOmPPu5rTceDxOhi+Xl5dcXl5ydHSUtNqybNYWvOd52sJvgAr+gLAX3kyn05UuvS34+/fvO9+B\nxlxB1+v1kgtAVgt/cnLCnTt38DzP6XjmCjq5mLXb7ZWFN9LCX15ecv/+/WSJrLTsEpGXc5OAnjnG\nV8Gno4K/hbLda+Ym01Dmlra23rULa563aTqRaLwE6aQFNcfxrsjaAXu1nSzAMU0vcn5pXgK7XNIW\nJKnLbhUV/Brq7l7zfZ+zszNOT0955JFHknG67/vJijZZyipmlF0EYa6EM+22chEZDAZLFzazHNVl\nlw8q+DWYXeyy3WthGCZTUeaYut/vEwRBcoxOp+Mcpfc8j9PTU05PTzk5OVkRvQhe5uGLErzneQwG\ng8ylsldXV4l7zvf95JzSxv7S/Z/NZlu77OSYKvgGU2f3Wr/fT4R+cnKS2sJ3u92lFn4XTMGbATdZ\nRZc1z75YLNRllxMq+DXU3b3W7/eTllNa9yAICIKgkBZezlsEJ4E36cabwyFznn0+n6vLLidU8Guo\nu3tN5uHtLQiClTF82i2ptsXu0puOtzQjjAh0Pp+ryy4nVPBrqLt7rdvtJq25tOzyPKuF3wW7S28v\nhwWWZgpE8LPZTF12OaGCv4U6u9fEhSaiMB/TovR5BLTSjC72mN1s3WVBjbrs8kEFv4a6u9c6nc5S\nV9fu+krQrqgWvtfrpY7Z5T3P85JYhrrs8kEFv4a6u9fMyr5uK2oeXsQn3Xhzqs4OiKrLLh92Efxj\nwHuBLyG+T/0PA/8mj5M6JOrsXjMrvT11Zb5mBsny6NKL4GG5Gz+bzRLXmznVKbERddntzi6CnwL/\nDPg4cAx8FPgQNbp7bd3da2b3Om0hir1YJa8uvVygROzS/ba76LI/Go3UZZcTuwj+C9cbwENioX8Z\nNRR8Xd1rWctM1215dOllXwTZ7XYzlywvFouktQZ12e1KXmP4x4GvBj6S0/87COruXgNWov1p+2nv\nuR7LnF6DVeORvX91dZV8P3XZ7U4egj8GPgC8hbilTzikVFMurjdzbl268ea+TMWZU3PymisyJWYG\nn8wup70azRSR+Zrs24/7doW5XjSiKFoJIApZgdVWq5V022W6MQxDPM8jiiJ8309+P9/3k9+6ai67\nbVJN7Sr4LvCTwI8CH7TfNAW/T1xdb2EYcn5+zsXFRRKJHw6HifBNC2seLURWzEAq8HA4XFrbLpVs\nNBqpKyyFppSn3Zg+++yzmZ/dRfAt4hRTvw784A7/p3BcXW9hGHJxcZEE5GzRm+vdixS8tG72fLh8\n3vd9dYWloOW5yi6C/4vA3wV+FfjY9WtvB35215MqAhfXWxRFS9NtaS286d/OI8qbNg24roLO53OC\nIFBXWAZansvsIvj/DVQiPaer6y2KoqRVl0dznF5UC2/e0WU8HifdTvtuMPJZ2xmmrrAYLc9VGrHS\nztX1Zi6XNbcwDFfG8BJUy7NLPx6Pl7qOZuU0u6lidFFX2DJanqs0SvDbut7SDDJmZN5u4Xf90bOm\nAc1upwQU7bUB6gpbRctzlUYIHtxcb/ZUnH1RyDtKL+dpVj6JAMNNS2RPQYmlVV1hq2h5LtMIwbu6\n3szgnLmZr9kLN/I4T/PClDbGlPfk7q/muagr7AYtz1UaJfhtXW9hGKYG9dKCfEVMy0nllG6nebEy\ng0pRFKkrLAUtz1UaIXhwc71FUbQU4bWn7WTfXAmXR5deKigsdzvlDrX2dFEQBOoKy0DLc5lGCN7V\n9RZF0cqCnKz9PLv0csccqZydTmfpXm32wo/xeKyusBS0PFdplOC3db1FUbTWxZW1NHfX85T9+XzO\n0dER0+l0rYNtNBqpKywFLc9VGiV4F9eb6dqS/5Xm6LLf2+U8zYq6iXtN/PfqCltGy3OVRggeVkVv\njsPNWyiZLf8urrddzxXiCmdXJHNqSPbtHoZZ0fO6GO0b06Ri3s/PXPJq58Jbd6EGllp/+z2Zhzdd\ni3bcpmpihwYJvgrYjqx1N6gw3wuCoNQccWVj3wtv05x0nuc5uSTrjAr+gDAr9qZOrXa7TRAEpeaI\nK5sswd+Wk06Crps6JGG37L9VQAV/YJgVO82xlfbo+36pOeLKxhT8NjnpJEeAPaVq7ksQD24Ce3VG\nBX9AmHO9UrGzXFvmc7nHXZk54srGnAPfNCed53mZzsh2u720GEe69FUrl21RwR8QWV3XtAQR5hYE\nQak54srGLpdNc9KJwcleDp1144v5fF6pcnFBBX9ArAtOZaWD8jxvqTUvI0dc2dhd+k1z0nmel8y6\nyAVAysC2xs5ms9qLHVTwB0fWIo80Ucu+CLvMHHFlk2ZYscfsdk46mVo1ezfy/WUqTlZfVnWosy0q\n+AMibb223YpL8gmzG28G58rIEVc2aUG7tDG7nZPO/O5pN71Is87WHRX8AbGuS58WmJMtCIJSc8SV\njV0uInbpxmflpPN9f+2969JuilGlcnFhV8G3gV8GXgD+2u6no6zr0kvrLtNv8ij55crMEVc2puBh\ns5x0YRhu1LJX9ULowq6CfwvxbaofyeFcGs8mK8pE5LLI5vT0NLmtclk54srG7LbL801y0vX7/aQX\nYI7ZTYu0RO3lQlh3dhH8y4HXA/8S+I58TqfZZI3hpUsvLbwI/ezsjLOzM3zfX+v+ylqaWxWkXGR/\n05x0nucl31VeS7sngrbwm/Eu4K3ASU7n0niypp/MLr3Zwp+dnXHnzh183890fpn7ae9VASkXU/ib\nOBglyaZ97zq5F0JaMLNK5eKCq+DfAPwhcQKKJ7M+dEi55VzJEtBt77lgzq3bU232FJw93153XC9S\nMg8vU5MSvDTjGlUXexm55V4FvJG4S+8Rt/LvBf6++aFDyS3niqt7zbXi+L5fa9ebUgx2Y1pEbrnv\nvt4AngD+OZbY64Cre801+ON5Xq1db8r+yWsevraeQhf3mqvg+/1+rV1vyv7JQ/A/f73VDlf3mmti\nwX6/X3vXm7JfdKXdGlzdazJfvC29Xq/Wrjdl/6jg1+DqXpPVYNvS7XZr7XpT9o8K/hZc3GuuLbzc\nvabOrjdlv6jg1+DqXnNt4eUuLXV1vSn7RwW/Blf3Wq/XczqefbvlurnelP2jgr8FF/eaq+DNJbV1\ndb0p+0UFvwZX95qs4d4W8+JSR9ebsn9U8Gtwda95nrfT8erqelP2jwp+Dbu413Y5Zl1db8r+aYzg\nXXKTtVotda8ptaIRgnfNTdZqtdS9ptSKRgv+ttxkrVZL3WtKrWic4LfJTQaoe02pFY0QPLjlJmu1\nWupeU2pFIwTvmpus1Wqpe02pFY0T/Da5yVqtlrrXlFrRCMGDW24yQN1rSq1ohOBdc5MB6l5TakXj\nBL9NbrJWq6XuNaVW7CL4M+A/AH+S+CaWTwH/N4+TKgKX3GSAuteUWrGL4P818DPA37z+P4NczqgA\nXHOTAWsda+peU6qGq+BPgW8A3nT9fAZc5HJGBeCamwxuuv3qXlPqgKvgvwL4I+BHgFcCHyXOJBvm\ndF654pqbTP5W3WtKXXAVfAf4GuDbgF8CfhB4GniH+aFDyi2nYqw2WRfnde+NRiPG43EShJXYjLnJ\nWgxztWXV2Ca3nKsCXgb8AnFLD/D1xIJ/g/GZq6oWoHJYXF1dLQ217OdZ74VhyPn5Oefn59y/fz/Z\nl+fymv3e0dERd+/e5e7duzz66KPJvr3Z7x2KLfq6YUvVtmsL/wXgs8BXAr8FvAb4Ncf/pShrERGn\nBVbNzX49DEMuLi64uLjg8vKSBw8e8PDhQ4bDIVEUJa2/2dLXvZHaJUr/T4AfA3rAZ4A353JGipKC\nCN6eOl33GEVRIvTLy8slsUt337wPggRq68wugv8E8OfzOhFFycJs4Wez2dLiqOl0uvTcfC2KoqRV\nl8cwDAnDUFt4RTlURPDSck8mE8bjMePxOGmp07YwDBkOhytbGIZLAT3pFajgFeUAMFt4U/DSNbcf\nZTNbc9mX52ktfN3FDip4pSLYXXpp3dNELfsi7KyLgtnCa5deUQ6ErC69KfjhcJiM0aXrbgbnzM18\nTbrzGrRTlAMhrUsvrXUYhjx8+HApMCdbGIapQb20IJ+28IpyQKzr0kvrLtNv8hhFUdIrSJu2s1fe\nqeAV5QBYF7SLoigR/OXlZbLI5uLigiiKVhbkZO1rl15RDoS0MbzZpR8Oh0mrfnFxkSyRjaJorRsy\na2lunVHBKwePvfBmOp2udOltwd+/f58oilbcj+uckeZ+XVHBK5XAFr05Dpfgm1wEzGk5ZRm9RYui\nNAgVvKI0CBW8ojQIFbyiNAgVvKI0CBW8ojQIFbyiNAgVvKI0CBW8ojSIXQT/duI71X4SeB/Qz+WM\nFEUpDFfBPw78I+JkFH8KaAN/J6dzUhSlIFzX0l8CUyAA5tePn8vrpBRFKQbXFv4e8APAHwCfB86B\nD+d1UoqiFINrC/8K4NuJu/YXwE8A30ycmCLhkHLLVQWXHGp1Z5cccVlJQNe953ke/X6fXq9Hr9ej\n2+3S6XSWNkkPfggZg8vILfe3gdcC//D6+d8Dvg74VuMzmltuS1xzqNW9nKMocsoRNx6PV1J5r0vz\nLftBEHB2dsbZ2Rl37txJ9uW5vGa/53nevosKKCa33KeAfwH4wIg4t9wvOv4v5RrXHGp1vzXTaDRy\nyhEnIm6327Tb7aRltjf79SAIOD095fT0lJOTEx555BGOj48ZDAb4vp+0/mZLv+9WflNcBf8J4L3A\nLwML4FeAH87rpJqMSw61ugt+PB4754gTwUu3fJNH3/cToZ+cnCyJXbr73W6XbrebCL4q7HLHm++7\n3pSccM2hNp/P933qhTKZTJxyxEkLL+NuGZObY/O050EQcHx8zPHxcdK6B0FAEASNbeGVAnDNoTab\nzfZ96oUymUyccsSZXXoRc7/fp9/vJy112hYEAYPBYGULgmApoCe9AhW84oRrDrXpdLrvUy+U6XTq\nlCMuS/DSNbcfZTNbc9mX51ktfFVQwR8YLjnU6t7CS673bXPEmYKXLr207mmiln0RdtZFwWzh2+32\n0rTeoaOCPyBcc6jVvYWfzWZOOeLMMby08HYrPhgMkjG6dN3N4FzWEEC689rCK8645lCbTCb7PvVC\nMS+A2+SIW9elTwvMyRYEwUpAL20zo/TawitOuORQq7vgzQQU2+aIW9ell9Zdpt/k0ff9pFeQNm1n\nr7xTwStOuOZQG4/H+z71QjEvgtvkiFvXwvu+z2AwSEQui2xOT0/xfX9lQU7WvnbpFWdcc6iNRqN9\nn3qhrFtivG7pcdYYXrr00sKL0GWJrO/7mUtw1y3NrQIq+ANilxxqdcclR5wdpe92uytderOFl/Xx\nvu8nkfcsg03We4dOYwRfBReaHYWWQJS5pa2tN8esh/z99oEtenMcLsE3uQiY03J1pRGCr4oLbTQa\ncXl5yXA4TKacZrMZV1dXSde01+vh+34ifvm7Knw/Zf80SvCH7kKbTCbJVFsYhsnqscViQavVotPp\nJEs/5dw6nQ6j0Wir7yZlooJvHo0QPFTDhSYryswlo9LCi+ClhYd4yqnb7Safs7+DuX90dJQs0Lm6\nuqq94UZJpxGCr4oLbT6fJ2N3c3242aXv9+ObA4vYPc/LPP/JZEK73WY6nSZBJSmLqgSZlHxplOAP\n3YUm52hvZpcebsQu3fSs72Mv/TQvfCr4ZtIowVfBhWYG1szpJmnhZRGJvL5YLJhOp8n593o9oihK\nPmu27NLDUbE3l0YIHqrjQpM5XVnMYW6dTif1PRn7mx5tEbtcGORiV7W130q+NELwVXGh2fPF9n7W\ne9PpNHFwyefsMbv53au0FFTJl0YJ/tBdaO12O1kIIo9A0iJL0M78TK/XYz6fp96QwezGy6o9beGb\nzW2CfzfwV4E/JE4pBXAX+HHgjwG/B/wt4kQUB00VXGgSdZcVX3AToLOn5cxtPp9v1LJX0c6p5Mtt\ngv8R4N8S36FWeBr4EPENLN92/fzpQs4uJ6riQuv1egwGgyQqL2K3p+V830/upHp8fMx8Pk9MHOaY\n3by4SdRe7JxKM7lN8P+LOLuMyRuBJ6733wM8T0UEf+guNM/zkrl/U+DmtJx5V9WTkxNOTk5YLBZJ\nqy3LZtN6M9rCKy5j+JcCL17vv3j9/KCpigvNXEEnVk65KWNaC39ycsKdO3eSFYHSsktPRnoxabdk\nUsE3k12DdlfXWypF5JZzcYWZc+vSjTf3zTug2ndFdclNtst3s++4mnZfddPd5fs+V1dXSbddvlsQ\nBIxGI3zfJ4qi5LNRFCX3ddtlLX1V3HmmWUgu+OZmrkqURmCdLdZ+zON335Vtcsu5CP5F4GXAF4Av\nJQ7opWIKPg9cXW9hGHJ+fs7FxcVKmqJN7nq6bW4y1x/f933Ozs44PT1dSW+UdrdU81i33dnFtNeK\n+FxtoFVxH2bFbuRiPxwOlzLISFmORqO1v2/ev/uu2I3ps88+m/lZF8H/FPAm4Lnrxw86/A8nXF1v\nYRiWmpvMNSjmeZ5zTrM0wXuex2AwyLybq2tQctvfQH67QxG8LFLKWnost7ja9DeH6twE4zbBv584\nQPcS4LPAO4B3Av8Z+AfcTMuVhovrLYqiUnOTuQq+3+875TTLuv+67/srFzL5fLvddha8GQs5dHde\nWgBzneDn8zlBEKT+tub+YrGg2+0CN+VZBW4T/DdlvP6avE9kE1xdb1EUlZqbzPXH7/f7TjnNzKCe\nXBD6/X5yETO70+Z3cl1nIIadQ3fn2bMzEsiUMjJ7SOZnR6NR5u88n8+TqVK4Kc+q3FugUivtXF1v\n5nLZMnKTiattW2Qe3iWnmXme/X5/qUttVk7zu7guHZ7NZpVw59ldevMc04xFUqfk4mr/zllDo3a7\nrYIvAlfXW5pBpsjcZNLV25Zut+uU0yzrZo1m5TQj/vJdXM1BVXHnZU3HmmUnAUV7jYb8rnIBSKsb\nUt5VETtUTPDg5nqzp+KKzk3m2sJLbvJtc5rZXfper5dUZLipnOaFy/M85xa+Su68NOOQPWa3PRby\nW5q9PhG7lLmUpX0T0UOnUoJ3db2ZwbkycpO5tvCdTscpp1nahSltzC7veZ630w0+JpNJJdx5aTGf\ntPOU90ajEf1+f6lOpA2L0i6uVaGSgt/W9RaGYam5yXq9ntP3MyvRtjnNzPOUyimtmXnu5vd1jZxP\nJpNKuPPs+iLnIL0fs9EwyzeKorWzG2nDJm3hC8LF9RZFUam5yVwFb1ambXKaSZdeBA/L3fjZbJYE\n6czv7ip4EXIV3Hmm4OU8pdWWmQq7rIMg2KhltxuIKlApwbu63qIoWlkMUmRuMvGxb4t5cdkmp5lZ\nGeW5CN289539XV27ouZKtEN255kXIXkuQp9Op5llOx6Pk16AOWY3L/oStZcGoipUUvDbut6iKMpc\n+rluSahrbjLP85y+37qlu+uW9AJL4m+328nCkNu+owtRFFXCnSf1RfbFRjydTteWqST2kJ5TVi9P\nW/iC2cX1Zt4QUv6XvW8/z5ru2iQ3mSvr8pate89c1531/dK+qwthGCb/65DdeVJfTOFvUp6yAtGe\n3ZDeXVqQVwVfELbozXG47Xoyp+Vucz+l/fjm3Lp04819c47cnjcvG9e13C6uN7uHIRearIvRPnG9\nuMnvLFO2Zp4/qXdVEztUUPAuuLregiBwdq9VgbLdh8r+aZTgt3W9BUHg7F6rAmW7D5X90wjBg5vr\nzfd9J/dalSjbfajsl0YI3tX1lra4ZhP3WlUo232o7J9GCX5b15u5XHZb91oVKNt9qOyfRgv+Ntdb\nmkFmE/daVSjbfajsn0YIHtYvkc1yvdlTcZu616pEme5DZf80QvCurjczOLeNe60qlO0+VPZPowS/\nrestCAJn91oVKNt9qOyfTQSfll/u+4E3ABPgM8CbgYsiTjAvXFxvvu87udeqRNnuQ2W/bCL4tPxy\nP0ecV25BfBfbt3PA6aZcXW9yu+Jt3WtVoWz3obJ/NhF8Wn65Dxn7HwH+Rl4nVASurjff953da1Wg\nbPehsn/yGMM/RXz/+oNlF9ebq3utCpTtPlT2z66C/x7icfz70t4sIrfcrqQJ1HZ9pbXYWS478//u\nExfX2y4591zZpDzT3nPFpVzk0fb6H2pOuqJzywnfArweeHXWB/LOLedK1li1LjnGquJ6c3Utupan\na7lUrb7YjWneueUAXge8lTgNVblJ1B1YF5yqQ46xqrjeXF2LrsHQbctEyrLO9WUTwdv55b6XOCrf\n4yZ49wvAPy7iBPMibfqpTjnGquJ6c3EtugrejE1smwOvrvVlE8Gn5Zd7d94nUiR2NLpuOcaq4npz\ndS26imI+d8uBV+f60oiVdnYXrW45xqrienN1Lbpm8nHNgVfn+tI4wdcxx1hVXG+urkXXTD6uOfDq\nXF8aIXiof46xqrjeXFyLri38Ljnw6lpfGiH4tDFu2hisqjnGquJ6c3Uturbwrjnw6lxfGif4OuYY\nq4rrzdW16Jq6yzUHnrngpm71pRGCh/rnGKuK683Ftegq+F1y4NW1vjRC8OaPLM/lh6tDjrGquN5c\nXYuuufpcc+DVub40SvCyP5/XK8dYVVxvrq5F11x9rjnw6lxfGiV484fcxPVWlRxjVXG97eJadME1\nB16d60vlBG+aDqTimJu5ukkq07oKDSxdze33pBsmc8NhGOJ5HlEU4ft+0m32fT+Zzip7Lf0urrfb\n3F1plduVfeTqk267lEkQBIxGo+S3k+NFUZSc2y715dBz0lVK8JuMAc213+aPUKZrahf7qAtRFDm5\n3sp2r/m+X2quvrLrSxWoheA9z2MwGGQuYZTgVFmuKdcgkyuj0cjJ9Va2e83zvFJz9ZVdX6pAZQVv\nBkJkdVPW/GcYhqW6psoW/Hg8dna9lele6/f7pebq20d9OXQqJXhgKcorARGpzOa4yIwIe55XqmvK\ndd7Ylclk4uR6K9u91u/3S8/VV2Z9qQKVErzdRTOdSGkGBamgUunLck25LgV1ZTKZOLneynav9Xq9\nUnP1lV1fqkBlBZ+2TBFIAkv2XG+ZrilXQbgynU6dXG9lu9e63W6pufrKri9VoFKCB1Z+HDNCas9/\nyo8nlb9wXGdxAAAGEUlEQVQs11TZLby4wrZ1vWWNcYtyr3W73dJz9ZVdXw6dSgk+rYKmjcHkPc/z\nGI/HS3ngynBNld3Cy91WtnW9le1ek/FxWbn6yq4vVeC2mpmWZkr4TuKUUy8B7uV/aqvYXVD58aTV\nNbulZtDF9/1SXVNl36fMvm3ypq63st1r9sKoonP1lV1fqsBtgk9LMwXwGPBa4PeLOKl1mD8gLHfL\nJEJuT6eEYViqa6pswZuVcFvXW5nuNXMsXVauvrLry6Fzm+DT0kwB/Cvgu4D/mvcJrcPshslz+eG6\n3W6m06vf75fqmiq7i2cOO7ZxvZXtXjMvLmnlltcCH6Hs+lIFXAab3wi8APxqzudyK/IDyn673Wax\niG/7u87h5Xleqa6psq/265Z+rlsSWrZ7bd3S3SLKsuz6UgW2FXwAfDdxd14o7ZvKD2j+kJs4vaRF\nKss1tQ9cXG9lu9dgfZnlXZ5l15cqsK3gX0Hcxf/E9fOXAx8FvpY4sLfEMwXklnOtBK6uqW3Y1T6a\nB9u42vbhXiubMutLq9VKphbNKT1zkwtEnheJInPLfRJ4qfH8d4E/S0aU3hT8PnF1TVWpYrtQtnut\nKrjWl1artZfytBvTXXLLSZqpR4nTTL2DOHIvVMIi5Oqakhsa1JWy3WtVwbW+tFqtgy/P2wSflmbK\n5I/ndSJF4uqaqrvgy3avVQXX+gIcfHlWaqXdLri4piaTyZ7Pulj24V6rCi71pdVqHXx5NkLwrq4p\nWWxTV8p2r1UF1/rSarUOvjwbJ/htXFOy4KaulO1eqwqu9aXVah18eTZC8ODmmqp7C78P91pVcKkv\nwMGXZyME7+qaqnsLX7Z7rSq41hfg4MuzcYLfxjVVlfuUuVK2e60quNaXVqt18OXZCMGDm2uq7oLf\nh3utKrjUF+Dgy7MRgnd1TVXlXuOulO1eqwqu9QU4+PJslOBlf1PX1L7XxRdN2e61quBaX4CDL88i\nj3Z1SILZxE2W9rm6U6Z7rUq41pdDKM/r/5n6jxsjeEVpCusE34xBmaIogApeURrFXgS/qVlfj6fH\nq/vxyv5uKng9nh5vj8drhOAVRdkPKnhFaRBFTss9T3x7LEVRyuXngSf3fRKKoiiKoiiKotSC1wGf\nAn4beFvBx3oM+B/ArwH/D/inBR8PoA18DPjpEo51BnwA+A3g14GvK/h4bycuy08C7wPcksxl827g\nxev/L9wFPgT8FvBzxN+5yON9P3F5fgL4L8BpwccTvhNYEH/f2tAGPk2cvaYLfBz4qgKP9zLgz1zv\nHwO/WfDxAL4D+DHgpwo+DsB7gKeu9zvkWzltHgd+hxuR/zjwppyP8Q3AV7MsiO8jTlwKcQPxzoKP\n91puZq/eWcLxIG6YfpY4sUutBP8XiL+Y8PT1VhYfBF5d4P9/OfBh4C9RfAt/SizAsrhLfMG8Q3xx\n+WngNQUc53GWBfEpbrIdvez6eZHHM/nrwI+WcLyfAP40JQi+7Hn4LyfOYCO8cP1aGTxOfHX9SIHH\neBfwVuKuWdF8BfBHxJmAfgX498TJPoviHvADwB8AnwfOiS9uRfNS4m4w148vXfPZvHkK+JmCj1Fq\nNuayBb8vv+wx8Vj3LcDDgo7xBuKEmh+jnIy6HeBrgB+6fhxSbG/pFcC3E184v4y4TL+5wOOlcUV5\ndeh7gAlxrKIoJBvz9xqvFVp3yhb854jHK8JjxFe3IukCP0ncNftggcd5FfBG4m7Z+4G/DLy3wOO9\ncL390vXzDxALvyj+HPB/gC8CM+KA1qsKPJ7wInFXHuBLSclSXADfArye4i9oZjbm3+UmG/OXFHzc\n0ugAnyH+kj2KD9q1iEX3rgKPkcYTlBOl/5/AV17vPwM8V+CxXkk80+ETl+t7gG8t4DiPsxq0k9mc\np8k3iJZ2vNcRz0S8JOfjZB3PpHZBO4C/Qhz8+TTxNE+RfD3xePrjxF3tjxH/oEXzBOVE6V9J3MIX\nMYWUxndxMy33HuLeU568nzg+MCGO9byZWAAfpphpOft4TxFPF/8+N/Xlhwo43pib72fyO9RQ8Iqi\nKIqiKIqiKIqiKIqiKIqiKIqiKIqiKIpSef4/hHklgi1HoiIAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7fe2d22e57d0>"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "usps_Y = np.array(usps_Y.T, dtype=np.int64)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#priors\n",
      "usps_alpha, usps_beta, usps_gamma = 50., .5, .5\n",
      "usps_K = 40\n",
      "usps_N, usps_D = usps_Y.shape\n",
      "usps_niters = 1000\n",
      "print usps_alpha, usps_beta, usps_gamma\n",
      "print usps_K, usps_N, usps_D "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "50.0 0.5 0.5\n",
        "40 475 256\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%timeit usps_timeit_niters = 1\n",
      "gibbs_beta_bernoulli(usps_Y, usps_K, usps_alpha, usps_beta, usps_gamma, usps_timeit_niters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 313 ms per loop\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "usps_chain = gibbs_beta_bernoulli(usps_Y, usps_K, usps_alpha, usps_beta, usps_gamma, usps_niters)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "print usps_Y.shape\n",
      "print usps_chain.shape\n",
      "\n",
      "def posterior_thetas(Y, beta, gamma):\n",
      "    N, D = Y.shape\n",
      "    return (beta + Y.sum(axis=0)) / (beta + gamma + N)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(475, 256)\n",
        "(1000, 475)\n",
        "[ 0.00105042  0.00735294  0.02205882  0.05987395  0.13340336  0.23004202\n",
        "  0.30357143  0.30357143  0.21323529  0.09348739  0.0262605   0.00945378\n",
        "  0.00315126  0.00315126  0.00315126  0.00105042  0.00105042  0.03886555\n",
        "  0.11659664  0.25105042  0.40231092  0.56197479  0.6144958   0.59768908\n",
        "  0.5157563   0.3539916   0.17121849  0.05777311  0.01365546  0.00735294\n",
        "  0.00315126  0.00105042  0.00105042  0.05777311  0.15021008  0.29936975\n",
        "  0.46323529  0.53676471  0.52415966  0.49684874  0.51155462  0.47794118\n",
        "  0.3539916   0.15861345  0.06407563  0.01785714  0.00945378  0.00105042\n",
        "  0.0052521   0.06617647  0.15441176  0.2657563   0.37710084  0.35189076\n",
        "  0.33298319  0.33088235  0.37710084  0.46323529  0.43172269  0.26365546\n",
        "  0.10609244  0.03046218  0.00945378  0.00315126  0.00105042  0.04306723\n",
        "  0.10189076  0.18172269  0.23634454  0.20693277  0.15861345  0.19222689\n",
        "  0.27836134  0.40651261  0.44012605  0.31617647  0.14390756  0.04516807\n",
        "  0.0157563   0.00315126  0.00105042  0.01785714  0.04936975  0.09348739\n",
        "  0.10189076  0.07668067  0.06197479  0.1144958   0.23214286  0.40441176\n",
        "  0.42542017  0.31617647  0.16071429  0.05357143  0.01365546  0.0052521\n",
        "  0.00315126  0.01365546  0.01995798  0.03466387  0.02205882  0.03046218\n",
        "  0.06407563  0.12920168  0.24894958  0.40021008  0.40021008  0.29306723\n",
        "  0.17331933  0.06407563  0.02205882  0.0052521   0.00105042  0.00735294\n",
        "  0.01785714  0.0262605   0.04936975  0.07457983  0.10819328  0.20063025\n",
        "  0.31197479  0.40651261  0.36659664  0.28676471  0.17752101  0.07247899\n",
        "  0.03676471  0.00315126  0.00315126  0.02205882  0.05357143  0.08928571\n",
        "  0.14180672  0.19432773  0.28256303  0.35819328  0.42121849  0.43802521\n",
        "  0.36239496  0.28256303  0.17121849  0.07037815  0.04096639  0.0052521\n",
        "  0.00945378  0.10819328  0.18172269  0.2552521   0.33718487  0.41071429\n",
        "  0.46743697  0.50735294  0.53256303  0.48634454  0.41071429  0.28676471\n",
        "  0.17752101  0.09138655  0.05147059  0.00735294  0.03046218  0.25105042\n",
        "  0.33298319  0.42121849  0.50105042  0.55147059  0.58298319  0.59138655\n",
        "  0.58928571  0.53676471  0.45693277  0.3539916   0.22794118  0.13130252\n",
        "  0.06827731  0.0052521   0.07037815  0.34138655  0.45483193  0.52836134\n",
        "  0.55567227  0.57247899  0.58718487  0.59978992  0.6039916   0.56407563\n",
        "  0.51785714  0.43592437  0.29096639  0.19012605  0.10819328  0.00315126\n",
        "  0.05567227  0.35819328  0.51155462  0.54726891  0.55357143  0.55567227\n",
        "  0.53046218  0.53046218  0.53466387  0.49894958  0.47794118  0.45903361\n",
        "  0.37710084  0.26365546  0.11659664  0.00315126  0.03256303  0.27836134\n",
        "  0.4737395   0.52836134  0.55987395  0.53886555  0.51995798  0.48634454\n",
        "  0.43592437  0.37710084  0.33088235  0.32457983  0.33718487  0.26995798\n",
        "  0.16701681  0.00735294  0.01155462  0.16701681  0.31617647  0.44432773\n",
        "  0.4842437   0.4947479   0.41491597  0.33928571  0.25105042  0.2237395\n",
        "  0.20693277  0.23214286  0.24054622  0.19222689  0.1039916   0.00735294\n",
        "  0.00105042  0.01785714  0.07668067  0.16281513  0.22163866  0.21113445\n",
        "  0.15231092  0.11869748  0.07037815  0.05357143  0.05357143  0.05357143\n",
        "  0.05987395  0.04726891  0.00945378  0.00105042]\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython import parallel\n",
      "clients = parallel.Client()\n",
      "view = clients.load_balanced_view()\n",
      "view.block = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 277
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "params = [usps_Y, usps_K, usps_alpha, usps_beta, usps_gamma, usps_niters]\n",
      "def workfn(params):\n",
      "    import impl\n",
      "    return impl.gibbs_beta_bernoulli(*params)\n",
      "usps_chains = view.map(workfn, [params] * 30)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 285
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}