{
 "metadata": {
  "name": "",
  "signature": "sha256:6af8c15ba3195687dbdbd0093256b1a3a3c4a842b26550c822efafd37ac5f9aa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Gibbs sampling for the beta-bernoulli mixture model"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bayesian beta-bernoulli mixture model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a fixed $K$ and $D$, the beta-bernoulli mixture model is a generative model describing $D$-dimensional binary vectors ($y_i \\in \\{0,1\\}^{D}$) drawn from a $K$-component mixture.\n",
      "\n",
      "We can describe the probabilistic model as:\n",
      "\\begin{align*}\n",
      "  \\pi|\\alpha &\\sim \\text{Dirichlet}(\\{ \\frac{\\alpha}{K} \\}_{i=1}^{K}) \\\\\n",
      "  c_i|\\pi &\\sim \\text{Discrete}(\\pi) \\\\\n",
      "  p_d^{k} | \\beta,\\gamma &\\sim \\text{Beta}(\\beta, \\gamma) \\\\\n",
      "  y_i^{d} | c_i{=}k,p_d^{k} &\\sim \\text{Bernoulli}(p_d^{k})\n",
      "\\end{align*}\n",
      "Let us clear up some of this notation. $\\pi$ is a $K$-dimensional vector living in the $(K-1)$-dimensional probability simplex. For each $k \\in [K]$, $\\{p_d^k\\}_{d=1}^{D} \\in [0,1]^{D}$. Given $N$ data points, the assignment vector $\\{c_i\\}_{i=1}^{N} \\in \\{0, ..., K-1\\}^{N}$. The hyperparameters of this model are $\\mathcal{H} = (\\alpha, \\beta, \\gamma)$.\n",
      "\n",
      "As we will see later on, this model has nice analytical properties for Gibbs sampling, since the beta distribution is a nice conjugate prior for the bernoulli distribution.\n",
      "\n",
      "The inference problem we will consider is the following. Given a dataset $\\mathcal{Y} = \\{y_i\\}_{i=1}^{N}$, we want to learn the posterior distribution on the assignment vector $\\mathcal{C} = \\{c_i\\}_{i=1}^{N}$. That is, we want to be able to estimate and draw samples from $p(\\mathcal{C} | \\mathcal{Y}; \\mathcal{H})$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Gibbs sampling for estimating the posterior"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This problem can be solved efficiently with [Gibbs sampling](http://en.wikipedia.org/wiki/Gibbs_sampling), which is another [Markov-chain monte carlo](http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) (MCMC) variant. Let the notation $\\mathcal{C}_{\\neg i} = \\{ c_j \\in \\mathcal{C} : j \\neq i \\}$. The Gibbs sampler assumes we can efficiently sample from the distribution $p(c_i | \\mathcal{C}_{\\neg i}, \\mathcal{Y}; \\mathcal{H})$. Every iteration of the sampling then works by\n",
      "sampling for each $i \\in [N]$,\n",
      "\\begin{align*}\n",
      "    c^{(t)}_{i} \\gets p(c_i | \\{ c_j \\in \\mathcal{C}^{(t)} : j < i \\}, \\{ c_j \\in \\mathcal{C}^{(t-1)} : j > i \\}, \\mathcal{Y}; \\mathcal{H})\n",
      "\\end{align*}\n",
      "\n",
      "Let the notation $\\mathcal{Y}^{k} = \\{ y_i \\in \\mathcal{Y} : c_i = k \\}$ and $\\mathcal{Y}^{k}_{\\neg i} = \\{ y_j \\in \\mathcal{Y} : c_j = k, j \\neq i \\}$.\n",
      "It turns out, for the beta-bernoulli model, we can indeed [derive](http://homepage.tudelft.nl/19j49/Publications_files/TR_1.pdf) that\n",
      "\\begin{align*}\n",
      "  p(c_i{=}k|\\mathcal{C}_{\\neg i}, \\mathcal{Y};\\mathcal{H}) \\propto\n",
      "    \\frac{ |\\mathcal{Y}^{k}_{\\neg i}| + \\frac{\\alpha}{K} }{ N - 1 + \\alpha } \\prod_{d=1}^{D} \\frac{(\\beta+\\sum_{y_k\\in \\mathcal{Y}^{k}} y_k^{d})^{y_i^d} (  \\gamma + |\\mathcal{Y}^{k}| - \\sum_{y_k\\in \\mathcal{Y}^{k}} y_k^{d})^{(1-y_i^{d})} }{ \\beta + \\gamma + |\\mathcal{Y}^{k}| }\n",
      "\\end{align*}\n",
      "We can construct $p(c_i{=}k|\\mathcal{C}_{\\neg i}, \\mathcal{Y};\\mathcal{H})$ exactly by then enumerating through all $K$ clusters and normalizing."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Implementation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def discrete_sample(pmf):\n",
      "    # XXX: does numpy have something to do this?\n",
      "    coin = np.random.random()\n",
      "    cdf = np.cumsum(pmf)\n",
      "    a = np.where(coin >= cdf)[0]\n",
      "    if not a.shape[0]:\n",
      "        return 0\n",
      "    return min(a[-1]+1, pmf.shape[0]-1)\n",
      "\n",
      "def gibbs_beta_bernoulli(Y, K, alpha, beta, gamma, niters):\n",
      "    N, D = Y.shape\n",
      "    alpha, beta, gamma = map(float, [alpha, beta, gamma])\n",
      "\n",
      "    # start with random assignment\n",
      "    assignments = np.random.randint(0, K, size=N)\n",
      "\n",
      "    # initialize the sufficient statistics (cluster sums) accordingly\n",
      "    sums = np.zeros((K, D)) \n",
      "    cnts = np.zeros(K)\n",
      "    for yi, ci in zip(Y, assignments):\n",
      "        sums[ci] += yi\n",
      "        cnts[ci] += 1\n",
      "\n",
      "    history = np.zeros((niters, N)) \n",
      "    for t in xrange(niters):\n",
      "        for i, (yi, ci) in enumerate(zip(Y, assignments)):\n",
      "            # remove from SS\n",
      "            sums[ci] -= yi\n",
      "            cnts[ci] -= 1\n",
      "\n",
      "            # build P(c_i=k | c_{\\i}, Y)\n",
      "            def fn(k):\n",
      "                lg_term1 = np.log(cnts[k] + alpha/K)\n",
      "                lg_term2 = np.log(N - 1 + alpha)\n",
      "                lg_term3 = D*np.log(beta + gamma + cnts[k] + 1) \n",
      "                def fn1(tup):\n",
      "                    d, yid = tup \n",
      "                    if yid:\n",
      "                        return np.log(beta + sums[k, d] + 1)\n",
      "                    else:\n",
      "                        return np.log(gamma + cnts[k] - sums[k, d]) \n",
      "                lg_term4 = sum(map(fn1, enumerate(yi)))\n",
      "                return np.exp(lg_term1 - lg_term2 - lg_term3 + lg_term4)\n",
      "\n",
      "            dist = np.array(map(fn, xrange(K)))\n",
      "            dist /= dist.sum()\n",
      "\n",
      "            # reassign\n",
      "            ci = discrete_sample(dist)\n",
      "            assignments[i] = ci\n",
      "            sums[ci] += yi\n",
      "            cnts[ci] += 1\n",
      "        history[t] = assignments\n",
      "\n",
      "    return history"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us first generate a small toy dataset (using the beta-bernoulli model as the generative process). Then we will test our Gibbs sampler on this small dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha, beta, gamma = 1., 1., 2.\n",
      "K = 2\n",
      "D = 3\n",
      "N = 5\n",
      "\n",
      "pis = np.random.dirichlet(alpha/K*np.ones(K))\n",
      "cis = np.array([discrete_sample(pis) for _ in xrange(N)])\n",
      "aks = np.random.beta(beta, gamma, size=(K, D))\n",
      "\n",
      "def bernoulli(p):\n",
      "    return 1. if np.random.random() <= p else 0.\n",
      "\n",
      "Y = np.zeros((N, D))\n",
      "for i in xrange(N):\n",
      "    Y[i] = np.array([bernoulli(aks[cis[i], d]) for d in xrange(D)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "niters = 50000\n",
      "chain = gibbs_beta_bernoulli(Y, K, alpha, beta, gamma, niters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the natural question is, how do we verify that the Gibbs sampler is indeed drawing samples from $p(\\mathcal{C} | \\mathcal{Y})$. This is a bit trickier in the Bayesian case than the non-Bayesian case, since what we are really after is a *distribution* instead of, e.g. a point estimate.\n",
      "\n",
      "Luckily, for small problems (small $K$ and $N$), we can actually calculate $p(\\mathcal{C} | \\mathcal{Y}; \\mathcal{H})$ exactly by brute force enumeration. This is because $p(\\mathcal{C} | \\mathcal{Y} ; \\mathcal{H})$ is a discrete distribution of size $K^{N}$, and we can analytically calculate the joint distribution $p(\\mathcal{C}, \\mathcal{Y}; \\mathcal{H})$; from the joint, the posterior follows by $p(\\mathcal{C} | \\mathcal{Y} ) = \\frac{p(\\mathcal{C}, \\mathcal{Y})}{\\sum_{c} p(c ,\\mathcal{Y})}$, where the summation in the denominator is over all possible $K^N$ assignment vectors (from here on, we drop the hyperparameter dependence to ease notation).\n",
      "\n",
      "For an arbitrary $\\mathcal{C}, \\mathcal{Y}$, we have $p(\\mathcal{C}, \\mathcal{Y}) = p(\\mathcal{C}) p(\\mathcal{Y} | \\mathcal{C})$. From [page 2](http://homepage.tudelft.nl/19j49/Publications_files/TR_1.pdf), we have \n",
      "\\begin{align*}\n",
      "  p(\\mathcal{C}) = \\frac{\\Gamma(\\alpha)}{\\Gamma(|\\mathcal{Y}|+\\alpha)}\n",
      "      \\prod_{k=1}^{K} \\frac{\\Gamma( |\\mathcal{Y}^{k}| + \\frac{\\alpha}{K})}{ \\Gamma(\\frac{\\alpha}{K})}\n",
      "\\end{align*}\n",
      "where $\\Gamma(\\cdot)$ is the [Gamma](http://en.wikipedia.org/wiki/Gamma_function) function.\n",
      "We can derive $p(\\mathcal{Y}|\\mathcal{C})$ as follows.\n",
      "\\begin{align*}\n",
      "  p(\\mathcal{Y} | \\mathcal{C}) &= \n",
      "    \\prod_{k=1}^{K} p(\\mathcal{Y}^{k} | \\mathcal{C}^{k}) \\\\\n",
      "    &= \\prod_{k=1}^{K} \\int_{\\Theta_k} [\\prod_{y_i \\in \\mathcal{Y}^k} p(y_i | \\Theta_k) ] p(\\Theta_k ; \\mathcal{H}) \\; d\\Theta_k \n",
      "\\end{align*}\n",
      "Now for each $k$, we evaluate the inner integral as:\n",
      "\\begin{align*}\n",
      "  \\int_{\\theta_1,...,\\theta_D} \\prod_{d=1}^{D} \\left(\\prod_{y_i \\in \\mathcal{Y}^k} \\theta_d^{y_i^d} (1-\\theta_d)^{1-y_i^d}\\right) \\frac{1}{B(\\beta,\\gamma)} \\theta_d^{\\beta-1}(1-\\theta_d)^{\\gamma-1} \\; d\\theta_1...\\theta_{D}\n",
      "\\end{align*}\n",
      "where $B(\\beta,\\gamma)$ is the [Beta function](http://en.wikipedia.org/wiki/Beta_function). Noting that $\\int_{0}^{1} x^{(m-1)} (1-x)^{(n-1)} \\; dx = \\frac{\\Gamma(m)\\Gamma(n)}{\\Gamma(m+n)}$, we can simplify the above integral to:\n",
      "\\begin{align*}\n",
      "\\frac{1}{B(\\beta,\\gamma)^{D}} \\prod_{d=1}^{D} \\frac{\\Gamma( \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\beta) \\Gamma( |\\mathcal{Y}^{k}| - \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\gamma)}{ \\Gamma( |\\mathcal{Y}^{k}| + \\beta + \\gamma )}\n",
      "\\end{align*}\n",
      "And therefore,\n",
      "\\begin{align*}\n",
      "  p(\\mathcal{Y} | \\mathcal{C}) = \\frac{1}{B(\\beta,\\gamma)^{KD}} \\prod_{k=1}^{K} \\prod_{d=1}^{D} \\frac{\\Gamma( \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\beta) \\Gamma( |\\mathcal{Y}^{k}| - \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\gamma)}{ \\Gamma( |\\mathcal{Y}^{k}| + \\beta + \\gamma )}\n",
      "\\end{align*}\n",
      "\n",
      "Therefore, we can compute the posterior distribution of the data as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy as sp\n",
      "import scipy.special\n",
      "import itertools as it\n",
      "import math\n",
      "\n",
      "def pr_joint(C, Y, K, alpha, beta, gamma):\n",
      "    N, D = Y.shape\n",
      "    nks = np.bincount(C, minlength=K)\n",
      "    assert nks.shape[0] == K\n",
      "    assert C.shape[0] == N\n",
      "\n",
      "    # log P(C)\n",
      "    gammaln = sp.special.gammaln\n",
      "    betaln = sp.special.betaln\n",
      "    term1 = gammaln(alpha) - gammaln(N + alpha) - K*gammaln(alpha/K)\n",
      "    term2 = sum(gammaln(nk + alpha/K) for nk in nks)\n",
      "    lg_pC = term1 + term2\n",
      "\n",
      "    # log P(Y|C)\n",
      "    term1 = K*D*betaln(beta, gamma)\n",
      "    term2 = D*sum(gammaln(nk + beta + gamma) for nk in nks)\n",
      "    sums = np.zeros((K, D))\n",
      "    for yi, ci in zip(Y, C):\n",
      "        sums[ci] += yi\n",
      "    def fn1(nk, sum_yid):\n",
      "        assert nk >= sum_yid\n",
      "        return gammaln(sum_yid + beta) + gammaln(nk - sum_yid + gamma)\n",
      "    term3 = sum(sum(fn1(nk, yid) for yid in row) for nk, row in zip(nks, sums))\n",
      "    lg_pYgC = -term1 - term2 + term3\n",
      "    \n",
      "    return np.exp(lg_pC + lg_pYgC)\n",
      "\n",
      "def brute_force_posterior(Y, K, alpha, beta, gamma):\n",
      "    N, _ = Y.shape\n",
      "\n",
      "    # enumerate K^N cluster assignments\n",
      "    pis = np.array([pr_joint(np.array(C), Y, K, alpha, beta, gamma) for C in it.product(range(K), repeat=N)])\n",
      "    pis /= pis.sum()\n",
      "\n",
      "    return pis\n",
      "\n",
      "actual_posterior = brute_force_posterior(Y, K, alpha, beta, gamma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To measure the distance between the actual posterior distribution and that produced by our Gibbs sampler, we compare the KL-divergence of the actual and the empirical. Recall for discrete distributions the KL-divergence (relative entropy) is defined as\n",
      "\\begin{align*}\n",
      "  D(P||Q) = \\sum_{x} P(x) \\log\\frac{P(x)}{Q(x)}\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "smoothing = 1e-5\n",
      "skip = 100\n",
      "\n",
      "def kl(a, b):\n",
      "    return np.sum([p*np.log(p/q) for p, q in zip(a, b)])\n",
      "\n",
      "def histify(history, K):\n",
      "    _, N = history.shape\n",
      "    # generate an ID for each K^N element\n",
      "    idmap = { C : i for i, C in enumerate(it.product(range(K), repeat=N)) }\n",
      "    hist = np.zeros(K**N, dtype=np.float)\n",
      "    for h in history:\n",
      "        hist[idmap[tuple(h)]] += 1.0\n",
      "    return hist\n",
      "\n",
      "def fn(i):\n",
      "    hist = histify(chain[:i:skip], K) + smoothing\n",
      "    hist /= hist.sum()\n",
      "    return kl(actual_posterior, hist)\n",
      "\n",
      "iters = range(0, niters, skip)[1:]\n",
      "kls = map(fn, iters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pylab as plt\n",
      "\n",
      "plt.plot(iters, kls)\n",
      "plt.xlabel('Iterations')\n",
      "plt.ylabel('KL-divergence')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "<matplotlib.text.Text at 0x10a632e50>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEPCAYAAABCyrPIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10a5f4950>"
       ]
      }
     ],
     "prompt_number": 9
    }
   ],
   "metadata": {}
  }
 ]
}