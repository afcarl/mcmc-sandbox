{
 "metadata": {
  "name": "",
  "signature": "sha256:c9837c467f9a5656b665ff6a4f4328398e295849618c6f0c46194344e9f81667"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Gibbs sampling for the beta-bernoulli mixture model"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bayesian beta-bernoulli (fixed size) mixture model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a fixed $K$ and $D$, the beta-bernoulli mixture model is a generative model describing $D$-dimensional binary vectors ($y_i \\in \\{0,1\\}^{D}$) drawn from a $K$-component mixture.\n",
      "\n",
      "We can describe the probabilistic model as:\n",
      "\\begin{align*}\n",
      "  \\pi|\\alpha &\\sim \\text{Dirichlet}(\\{ \\frac{\\alpha}{K} \\}_{k=1}^{K}) \\\\\n",
      "  c_i|\\pi &\\sim \\text{Discrete}(\\pi) \\\\\n",
      "  p_d^{k} | \\beta,\\gamma &\\sim \\text{Beta}(\\beta, \\gamma) \\\\\n",
      "  y_i^{d} | c_i{=}k,p_d^{k} &\\sim \\text{Bernoulli}(p_d^{k})\n",
      "\\end{align*}\n",
      "Let us clear up some of this notation. $\\pi$ is a $K$-dimensional vector living in the $(K-1)$-dimensional probability simplex. For each $k \\in [K]$, $\\{p_d^k\\}_{d=1}^{D} \\in [0,1]^{D}$. Given $N$ data points, the assignment vector $\\{c_i\\}_{i=1}^{N} \\in \\{0, ..., K-1\\}^{N}$. The hyperparameters of this model are $\\mathcal{H} = (\\alpha, \\beta, \\gamma)$.\n",
      "\n",
      "As we will see later on, this model has nice analytical properties for Gibbs sampling, since the beta distribution is a nice conjugate prior for the bernoulli distribution.\n",
      "\n",
      "The inference problem we will consider is the following. Given a dataset $\\mathcal{Y} = \\{y_i\\}_{i=1}^{N}$, we want to learn the posterior distribution on the assignment vector $\\mathcal{C} = \\{c_i\\}_{i=1}^{N}$. That is, we want to be able to estimate and draw samples from $p(\\mathcal{C} | \\mathcal{Y}; \\mathcal{H})$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Gibbs sampling for estimating the posterior"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This problem can be solved efficiently with [Gibbs sampling](http://en.wikipedia.org/wiki/Gibbs_sampling), which is another [Markov-chain monte carlo](http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) (MCMC) variant. Let the notation $\\mathcal{C}_{\\neg i} = \\{ c_j \\in \\mathcal{C} : j \\neq i \\}$. The Gibbs sampler assumes we can efficiently sample from the distribution $p(c_i | \\mathcal{C}_{\\neg i}, \\mathcal{Y}; \\mathcal{H})$. Every iteration of the sampling then works by\n",
      "sampling for each $i \\in [N]$,\n",
      "\\begin{align*}\n",
      "    c^{(t)}_{i} \\gets p(c_i | \\{ c_j \\in \\mathcal{C}^{(t)} : j < i \\}, \\{ c_j \\in \\mathcal{C}^{(t-1)} : j > i \\}, \\mathcal{Y}; \\mathcal{H})\n",
      "\\end{align*}\n",
      "\n",
      "Let the notation $\\mathcal{Y}^{k}_{\\neg i} = \\{ y_j \\in \\mathcal{Y} : c_j = k, j \\neq i \\}$.\n",
      "It turns out, for the beta-bernoulli model, we can indeed [derive](http://homepage.tudelft.nl/19j49/Publications_files/TR_1.pdf) that\n",
      "\\begin{align*}\n",
      "  p(c_i{=}k|\\mathcal{C}_{\\neg i}, \\mathcal{Y};\\mathcal{H}) \\propto\n",
      "    \\frac{ |\\mathcal{Y}^{k}_{\\neg i}| + \\frac{\\alpha}{K} }{ N - 1 + \\alpha } \\prod_{d=1}^{D} \\frac{(\\beta+\\sum_{y_k\\in \\mathcal{Y}^{k}_{\\neg i}} y_k^{d})^{y_i^d} (  \\gamma + |\\mathcal{Y}^{k}_{\\neg i}| - \\sum_{y_k\\in \\mathcal{Y}^{k}_{\\neg i}} y_k^{d})^{(1-y_i^{d})} }{ \\beta + \\gamma + |\\mathcal{Y}^{k}_{\\neg i}| }\n",
      "\\end{align*}\n",
      "We can construct $p(c_i{=}k|\\mathcal{C}_{\\neg i}, \\mathcal{Y};\\mathcal{H})$ exactly by then enumerating through all $K$ clusters and normalizing."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Implementation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll first implement a sampler from a discrete distribution. Note while `scipy.stats.rv_discrete` [(Docs)](http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_discrete.html) implements this functionality, it is quite heavy weight to call within a loop, so we roll our own."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as sp\n",
      "import scipy.misc\n",
      "import math\n",
      "\n",
      "def discrete_sample(pmf):\n",
      "    coin = np.random.random()\n",
      "    cdf = np.cumsum(pmf)\n",
      "    a = np.where(coin >= cdf)[0]\n",
      "    if not a.shape[0]:\n",
      "        return 0\n",
      "    return min(a[-1]+1, pmf.shape[0]-1)\n",
      "\n",
      "### Some test code for discrete_sample(), feel free to ignore ###\n",
      "def almost_eq(a, b, tol=1e-5):\n",
      "    return math.fabs(a - b) <= tol\n",
      "\n",
      "def pmfs_almost_eq(a, b, tol):\n",
      "    assert a.shape == b.shape\n",
      "    return (np.fabs(a - b) <= tol).all()\n",
      "\n",
      "def test_discrete_sample0(pmf, tol=1e-3):\n",
      "    samples = np.array(\n",
      "        np.bincount(\n",
      "            [discrete_sample(pmf) for _ in xrange(500000)], \n",
      "            minlength=pmf.shape[0]), \n",
      "        dtype=np.float)\n",
      "    samples /= samples.sum()\n",
      "    if not pmfs_almost_eq(pmf, samples, tol):\n",
      "        print 'max distance:', np.fabs(pmf - samples).max()\n",
      "        assert False\n",
      "    \n",
      "def test_discrete_sample():\n",
      "    pmf = np.arange(1, 6, dtype=np.float)\n",
      "    pmf /= pmf.sum()\n",
      "    test_discrete_sample0(pmf, 1e-2)\n",
      "    test_discrete_sample0(np.array([0.95, 0.05])) # binary case 1\n",
      "    test_discrete_sample0(np.array([0.01, 0.99])) # binary case 2\n",
      "    \n",
      "test_discrete_sample()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the actual Gibbs sampler implementation:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gibbs_beta_bernoulli(Y, K, alpha, beta, gamma, niters):\n",
      "    N, D = Y.shape\n",
      "    alpha, beta, gamma = map(float, [alpha, beta, gamma])\n",
      "\n",
      "    # start with random assignment\n",
      "    assignments = np.random.randint(0, K, size=N)\n",
      "\n",
      "    # initialize the sufficient statistics (cluster sums) accordingly\n",
      "    sums = np.zeros((K, D), dtype=np.int64) \n",
      "    cnts = np.zeros(K, dtype=np.int64)\n",
      "    for yi, ci in zip(Y, assignments):\n",
      "        sums[ci] += yi\n",
      "        cnts[ci] += 1\n",
      "    #assert cnts.sum() == N\n",
      "\n",
      "    history = np.zeros((niters, N), dtype=np.int64) \n",
      "    for t in xrange(niters):\n",
      "        for i, (yi, ci) in enumerate(zip(Y, assignments)):\n",
      "            # remove from SS\n",
      "            #assert cnts[ci] >= 1\n",
      "            sums[ci] -= yi\n",
      "            cnts[ci] -= 1\n",
      "\n",
      "            # build log P(c_i=k | c_{\\i}, Y)\n",
      "            def fn(k):\n",
      "                lg_term1 = np.log(cnts[k] + alpha/K)\n",
      "                lg_term2 = np.log(N - 1 + alpha)\n",
      "                lg_term3 = D*np.log(beta + gamma + cnts[k]) \n",
      "                def fn1(tup):\n",
      "                    d, yid = tup \n",
      "                    #assert yid == 0 or yid == 1\n",
      "                    #assert d >= 0 and d < D\n",
      "                    #assert cnts[k] >= sums[k, d]\n",
      "                    if yid:\n",
      "                        return np.log(beta + sums[k, d])\n",
      "                    else:\n",
      "                        return np.log(gamma + cnts[k] - sums[k, d]) \n",
      "                lg_term4 = sum(map(fn1, enumerate(yi)))\n",
      "                return lg_term1 - lg_term2 - lg_term3 + lg_term4\n",
      "\n",
      "            lg_dist = np.array(map(fn, xrange(K)))\n",
      "            lg_dist -= sp.misc.logsumexp(lg_dist) # normalize\n",
      "            dist = np.exp(lg_dist)\n",
      "            assert almost_eq(dist.sum(), 1.0)\n",
      "\n",
      "            # reassign\n",
      "            ci = discrete_sample(dist)\n",
      "            assignments[i] = ci\n",
      "            sums[ci] += yi\n",
      "            cnts[ci] += 1\n",
      "            \n",
      "        #assert cnts.sum() == N\n",
      "        history[t] = assignments\n",
      "\n",
      "    return history"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 86
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us first generate a small toy dataset (using the beta-bernoulli model as the generative process). Then we will test our Gibbs sampler on this small dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha, beta, gamma = 2., 1., 1.\n",
      "K = 2\n",
      "D = 3\n",
      "N = 4\n",
      "\n",
      "pis = np.random.dirichlet(alpha/K*np.ones(K))\n",
      "cis = np.array([discrete_sample(pis) for _ in xrange(N)])\n",
      "aks = np.random.beta(beta, gamma, size=(K, D))\n",
      "\n",
      "print 'Pi:', pis\n",
      "print 'C :', cis\n",
      "\n",
      "def bernoulli(p):\n",
      "    return 1 if np.random.random() <= p else 0\n",
      "\n",
      "Y = np.zeros((N, D), dtype=np.int64)\n",
      "for i in xrange(N):\n",
      "    Y[i] = np.array([bernoulli(aks[cis[i], d]) for d in xrange(D)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Pi: [ 0.43014276  0.56985724]\n",
        "C : [0 0 0 1]\n"
       ]
      }
     ],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "niters = 50000\n",
      "chain = gibbs_beta_bernoulli(Y, K, alpha, beta, gamma, niters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 110
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Posterior distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the natural question is, how do we verify that the Gibbs sampler is indeed drawing samples from $p(\\mathcal{C} | \\mathcal{Y})$. This is a bit trickier in the Bayesian case than the non-Bayesian case, since what we are really after is a *distribution* instead of, e.g. a point estimate.\n",
      "\n",
      "Luckily, for small problems (small $K$ and $N$), we can actually calculate $p(\\mathcal{C} | \\mathcal{Y}; \\mathcal{H})$ exactly by brute force enumeration. This is because $p(\\mathcal{C} | \\mathcal{Y} ; \\mathcal{H})$ is a discrete distribution of size $K^{N}$, and we can analytically calculate the joint distribution $p(\\mathcal{C}, \\mathcal{Y}; \\mathcal{H})$; from the joint, the posterior follows by $p(\\mathcal{C} | \\mathcal{Y} ) = \\frac{p(\\mathcal{C}, \\mathcal{Y})}{\\sum_{c} p(c ,\\mathcal{Y})}$, where the summation in the denominator is over all possible $K^N$ assignment vectors (from here on, we drop the hyperparameter dependence to ease notation).\n",
      "\n",
      "For an arbitrary $\\mathcal{C}, \\mathcal{Y}$, we have $p(\\mathcal{C}, \\mathcal{Y}) = p(\\mathcal{C}) p(\\mathcal{Y} | \\mathcal{C})$. From [page 2](http://homepage.tudelft.nl/19j49/Publications_files/TR_1.pdf), we have \n",
      "\\begin{align*}\n",
      "  p(\\mathcal{C}) = \\frac{\\Gamma(\\alpha)}{\\Gamma(|\\mathcal{Y}|+\\alpha)}\n",
      "      \\prod_{k=1}^{K} \\frac{\\Gamma( |\\mathcal{Y}^{k}| + \\frac{\\alpha}{K})}{ \\Gamma(\\frac{\\alpha}{K})}\n",
      "\\end{align*}\n",
      "where $\\Gamma(\\cdot)$ is the [Gamma](http://en.wikipedia.org/wiki/Gamma_function) function.\n",
      "We can derive $p(\\mathcal{Y}|\\mathcal{C})$ as follows.\n",
      "\\begin{align*}\n",
      "  p(\\mathcal{Y} | \\mathcal{C}) &= \n",
      "    \\prod_{k=1}^{K} p(\\mathcal{Y}^{k} | \\mathcal{C}^{k}) \\\\\n",
      "    &= \\prod_{k=1}^{K} \\int_{\\Theta_k} [\\prod_{y_i \\in \\mathcal{Y}^k} p(y_i | \\Theta_k) ] p(\\Theta_k ; \\mathcal{H}) \\; d\\Theta_k \n",
      "\\end{align*}\n",
      "Now for each $k$, we evaluate the inner integral as:\n",
      "\\begin{align*}\n",
      "  \\int_{\\theta_1,...,\\theta_D} \\prod_{d=1}^{D} \\left(\\prod_{y_i \\in \\mathcal{Y}^k} \\theta_d^{y_i^d} (1-\\theta_d)^{1-y_i^d}\\right) \\frac{1}{B(\\beta,\\gamma)} \\theta_d^{\\beta-1}(1-\\theta_d)^{\\gamma-1} \\; d\\theta_1...\\theta_{D}\n",
      "\\end{align*}\n",
      "where $B(\\beta,\\gamma)$ is the [Beta function](http://en.wikipedia.org/wiki/Beta_function). Noting that $\\int_{0}^{1} x^{(m-1)} (1-x)^{(n-1)} \\; dx = \\frac{\\Gamma(m)\\Gamma(n)}{\\Gamma(m+n)}$, we can simplify the above integral to:\n",
      "\\begin{align*}\n",
      "\\frac{1}{B(\\beta,\\gamma)^{D}} \\prod_{d=1}^{D} \\frac{\\Gamma( \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\beta) \\Gamma( |\\mathcal{Y}^{k}| - \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\gamma)}{ \\Gamma( |\\mathcal{Y}^{k}| + \\beta + \\gamma )}\n",
      "\\end{align*}\n",
      "And therefore,\n",
      "\\begin{align*}\n",
      "  p(\\mathcal{Y} | \\mathcal{C}) = \\frac{1}{B(\\beta,\\gamma)^{KD}} \\prod_{k=1}^{K} \\prod_{d=1}^{D} \\frac{\\Gamma( \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\beta) \\Gamma( |\\mathcal{Y}^{k}| - \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\gamma)}{ \\Gamma( |\\mathcal{Y}^{k}| + \\beta + \\gamma )}\n",
      "\\end{align*}\n",
      "\n",
      "Therefore, we can compute the posterior distribution of the data as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.special\n",
      "import itertools as it\n",
      "\n",
      "def all_assignment_vectors():\n",
      "    return it.product(range(K), repeat=N)\n",
      "\n",
      "def lg_pr_joint(C, Y, K, alpha, beta, gamma):\n",
      "    N, D = Y.shape\n",
      "    nks = np.bincount(C, minlength=K)\n",
      "    assert nks.shape[0] == K\n",
      "    assert C.shape[0] == N\n",
      "\n",
      "    # log P(C)\n",
      "    gammaln = sp.special.gammaln\n",
      "    betaln = sp.special.betaln\n",
      "    term1 = gammaln(alpha) - gammaln(N + alpha) - K*gammaln(alpha/K)\n",
      "    term2 = sum(gammaln(nk + alpha/K) for nk in nks)\n",
      "    lg_pC = term1 + term2\n",
      "\n",
      "    # log P(Y|C)\n",
      "    term1 = K*D*betaln(beta, gamma)\n",
      "    term2 = D*sum(gammaln(nk + beta + gamma) for nk in nks)\n",
      "    sums = np.zeros((K, D))\n",
      "    for yi, ci in zip(Y, C):\n",
      "        sums[ci] += yi\n",
      "    def fn1(nk, sum_yid):\n",
      "        assert nk >= sum_yid\n",
      "        return gammaln(sum_yid + beta) + gammaln(nk - sum_yid + gamma)\n",
      "    term3 = sum(sum(fn1(nk, yid) for yid in row) for nk, row in zip(nks, sums))\n",
      "    lg_pYgC = -term1 - term2 + term3\n",
      "    \n",
      "    return lg_pC + lg_pYgC\n",
      "\n",
      "def brute_force_posterior(Y, K, alpha, beta, gamma):\n",
      "    N, _ = Y.shape\n",
      "\n",
      "    # enumerate K^N cluster assignments\n",
      "    lg_pis = np.array([lg_pr_joint(np.array(C), Y, K, alpha, beta, gamma) for C in all_assignment_vectors()])\n",
      "    lg_pis -= sp.misc.logsumexp(lg_pis)\n",
      "    \n",
      "    return np.exp(lg_pis)\n",
      "\n",
      "# generate an ID for each K^N element\n",
      "idmap = { C : i for i, C in enumerate(all_assignment_vectors()) }\n",
      "revidmap = { i : C for i, C in enumerate(all_assignment_vectors()) }\n",
      "\n",
      "actual_posterior = brute_force_posterior(Y, K, alpha, beta, gamma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 123
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's pause for a second and take a look at the [MAP](http://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) estimator:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'P(C=actual|Y):', actual_posterior[idmap[tuple(cis)]]\n",
      "print 'max_C P(C|Y):', actual_posterior.max()\n",
      "print 'argmax_C P(C|Y):', revidmap[actual_posterior.argmax()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "P(C=actual|Y): 0.0864616067769\n",
        "max_C P(C|Y): 0.132805028009\n",
        "argmax_C P(C|Y): (0, 0, 0, 0)\n"
       ]
      }
     ],
     "prompt_number": 124
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice how the ground truth cluster assignment is *not* the MAP estimator!\n",
      "\n",
      "To measure the distance between the actual posterior distribution and that produced by our Gibbs sampler, we compare the KL-divergence of the actual and the empirical. Recall for discrete distributions the KL-divergence (relative entropy) is defined as\n",
      "\\begin{align*}\n",
      "  D(P||Q) = \\sum_{x} P(x) \\log\\frac{P(x)}{Q(x)}\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "smoothing = 1e-5\n",
      "skip = 100\n",
      "skipped_chain = chain[::skip]\n",
      "window = 10000\n",
      "\n",
      "def kl(a, b):\n",
      "    return np.sum([p*np.log(p/q) for p, q in zip(a, b)])\n",
      "\n",
      "def histify(history, K):\n",
      "    _, N = history.shape\n",
      "    hist = np.zeros(K**N, dtype=np.float)\n",
      "    for h in history:\n",
      "        hist[idmap[tuple(h)]] += 1.0\n",
      "    return hist\n",
      "\n",
      "def fn(i):\n",
      "    hist = histify(skipped_chain[max(0, i-window):(i+1)], K) + smoothing\n",
      "    hist /= hist.sum()\n",
      "    return kl(actual_posterior, hist)\n",
      "\n",
      "kls = map(fn, xrange(skipped_chain.shape[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 122
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's take a peek at the posterior distribution of the *last* window compared with our brute force posterior distribution:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "H = histify(skipped_chain[skipped_chain.shape[0]-1-window:], K) + smoothing\n",
      "H /= H.sum()\n",
      "print \"C\\t\\tP(C|Y) [gibbs]\\tP(C|Y) [actual]\\t|Diff|\"\n",
      "for c, (a, b) in zip(all_assignment_vectors(), zip(H, actual_posterior)):\n",
      "    print \"\\t\".join(map(str, [c,a,b,math.fabs(a-b)]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "C\t\tP(C|Y) [gibbs]\tP(C|Y) [actual]\t|Diff|\n",
        "(0, 0, 0, 0)\t0.14199997456\t0.132805028009\t0.00919494655072\n",
        "(0, 0, 0, 1)\t0.07399999632\t0.0864616067769\t0.0124616104569\n",
        "(0, 0, 1, 0)\t0.0300000104\t0.0288205355923\t0.0011794748077\n",
        "(0, 0, 1, 1)\t0.0200000136\t0.0227717812087\t0.00277176760873\n",
        "(0, 1, 0, 0)\t0.09399998992\t0.0864616067769\t0.00753838314312\n",
        "(0, 1, 0, 1)\t0.0200000136\t0.0227717812087\t0.00277176760873\n",
        "(0, 1, 1, 0)\t0.0899999912\t0.0910871248349\t0.0010871336349\n",
        "(0, 1, 1, 1)\t0.02600001168\t0.0288205355923\t0.0028205239123\n",
        "(1, 0, 0, 0)\t0.02400001232\t0.0288205355923\t0.0048205232723\n",
        "(1, 0, 0, 1)\t0.08799999184\t0.0910871248349\t0.0030871329949\n",
        "(1, 0, 1, 0)\t0.02400001232\t0.0227717812087\t0.00122823111127\n",
        "(1, 0, 1, 1)\t0.09199999056\t0.0864616067769\t0.00553838378312\n",
        "(1, 1, 0, 0)\t0.01800001424\t0.0227717812087\t0.00477176696873\n",
        "(1, 1, 0, 1)\t0.0300000104\t0.0288205355923\t0.0011794748077\n",
        "(1, 1, 1, 0)\t0.09799998864\t0.0864616067769\t0.0115383818631\n",
        "(1, 1, 1, 1)\t0.1299999784\t0.132805028009\t0.00280504960928\n"
       ]
      }
     ],
     "prompt_number": 134
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This looks reasonable. We can also get a sense for the trend as we increase the number of iterations:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pylab as plt\n",
      "\n",
      "plt.plot(np.arange(0, chain.shape[0], skip) + 1, kls)\n",
      "plt.xlabel('Iterations')\n",
      "plt.ylabel('Posterior KL-divergence')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 146,
       "text": [
        "<matplotlib.text.Text at 0x7f105ba51250>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEPCAYAAABCyrPIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f105babea50>"
       ]
      }
     ],
     "prompt_number": 146
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Posterior predictive distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Another distribution we can look at is the posterior *predictive* distribution $p(y|\\mathcal{C},\\mathcal{Y};\\mathcal{H})$, that is, the distribution over the *next* data point. To derive this, we first note that $p(y|\\mathcal{C},\\mathcal{Y}) = \\sum_{k=1}^{K} p(y,c{=}k|\\mathcal{C},\\mathcal{Y}) = \\sum_{k=1}^{K} p(c{=}k|\\mathcal{C}) p(y|\\mathcal{Y}_{k})$.\n",
      "\n",
      "From [Eq. 9, pg. 3](http://homepage.tudelft.nl/19j49/Publications_files/TR_1.pdf), we have $p(c{=}k|\\mathcal{C}) = \\frac{|\\mathcal{C}_{k}| + \\frac{\\alpha}{K}}{|\\mathcal{C}| + \\alpha}$. Next, $p(y|\\mathcal{Y}_{k})$ is simply the posterior predictive of the beta-bernoulli likelihood model. That is, $p(y_d{=}1|\\mathcal{Y}_{k})= \\frac{\\beta + \\sum_{y_i \\in \\mathcal{Y}_{k}} y_i^d}{ \\beta + \\gamma + |\\mathcal{Y}_{k}|  } $. Putting it together, we have\n",
      "\\begin{align*}\n",
      "  p(y|\\mathcal{C},\\mathcal{Y}) = \\sum_{k=1}^{K} \\frac{|\\mathcal{C}_{k}| + \\frac{\\alpha}{K}}{|\\mathcal{C}| + \\alpha} \\prod_{d=1}^{D} \\frac{\\beta + \\sum_{y_i \\in \\mathcal{Y}_{k}} y_i^d}{ \\beta + \\gamma + |\\mathcal{Y}_{k}|  } \n",
      "\\end{align*}\n",
      "\n",
      "We can also look at the non-Bayesian case, where we know the exact values for $\\pi$ and $\\{ p_d^k \\}$.\n",
      "\\begin{align*}\n",
      "  p(y | \\pi, \\{p_d^k\\}) = \\sum_{k=1}^{K} \\pi_k \\prod_{d=1}^{D} (p_d^k)^{y_d} (1-p_d^k)^{1-y_d}\n",
      "\\end{align*}\n",
      "\n",
      "This makes explicit how our prior information affects our estimates of $\\hat{\\pi}$ and $\\hat{p_d^k}$. Now for the reference distributions, we know both the ground truth cluster assignment and the exact model parameters, so we can compute $ p(y|\\mathcal{C},\\mathcal{Y}) $ and $ p(y | \\pi, \\{p_d^k\\})$ exactly. For our Gibbs sampler, we estimate $p(y|\\mathcal{C},\\mathcal{Y})$ by averaging over our samples of $\\mathcal{C}$. Details below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def posterior_predictive(C, Y, K, alpha, beta, gamma):\n",
      "    N, D = Y.shape\n",
      "    assert C.shape[0] == N\n",
      "\n",
      "    nks = np.bincount(C, minlength=K)\n",
      "    sums = np.zeros((K, D), dtype=np.int64) \n",
      "    for yi, ci in zip(Y, C): \n",
      "        sums[ci] += yi\n",
      "\n",
      "    def fn(yvalue):\n",
      "        def fn1(nk, sum_yid, yd):\n",
      "            assert nk >= sum_yid\n",
      "            theta = (beta + sum_yid) / (beta + gamma + nk) \n",
      "            assert theta >= 0.0 and theta <= 1.0 \n",
      "            return np.log(theta) if yd else np.log(1.-theta)\n",
      "        def fn2(nk, row):\n",
      "            assert len(yvalue) == row.shape[0]\n",
      "            term1 = np.log(nk + alpha/K) - np.log(N + alpha)\n",
      "            term2 = sum(fn1(nk, sum_yid, yd) for sum_yid, yd in zip(row, yvalue))\n",
      "            return term1 + term2\n",
      "        return sp.misc.logsumexp([fn2(nk, row) for nk, row in zip(nks, sums)])\n",
      "\n",
      "    yvalues = it.product([0, 1], repeat=D)\n",
      "    lg_pr_yvalue = map(fn, yvalues)\n",
      "    return np.exp(lg_pr_yvalue)\n",
      "\n",
      "def posterior_predictive_nonbayes(pis, aks):\n",
      "    K, = pis.shape\n",
      "    assert aks.shape[0] == K\n",
      "    _, D = aks.shape\n",
      "    \n",
      "    def fn(yvalue):\n",
      "        def fn2(yd, theta_kd):\n",
      "            return np.log(theta_kd) if yd else np.log(1.-theta_kd)\n",
      "        def fn1(pi_k, theta_k):\n",
      "            return np.log(pi_k) + sum(fn2(yd, theta_kd) for yd, theta_kd in zip(yvalue, theta_k))\n",
      "        return sp.misc.logsumexp([fn1(pi_k, theta_k) for pi_k, theta_k in zip(pis, aks)])\n",
      "    \n",
      "    yvalues = it.product([0, 1], repeat=D)\n",
      "    lg_pr_yvalue = map(fn, yvalues)\n",
      "    return np.exp(lg_pr_yvalue)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 162
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reference distributions\n",
      "actual_posterior_predictive = posterior_predictive(cis, Y, K, alpha, beta, gamma)\n",
      "nonbayes_posterior_predictive = posterior_predictive_nonbayes(pis, aks)\n",
      "\n",
      "posterior_predictives = np.array([\n",
      "    posterior_predictive(assignment, Y, K, alpha, beta, gamma) for assignment in skipped_chain])\n",
      "    \n",
      "def fn1(i):\n",
      "    posteriors = posterior_predictives[min(0, i-window):(i+1)].mean(axis=0)\n",
      "    return kl(actual_posterior_predictive, posteriors)\n",
      "\n",
      "posterior_predictive_kls = map(fn1, xrange(posterior_predictives.shape[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 163
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's take a look at the posterior predictive distributions of the *last* window compared to the ground truth distributions:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PH = posterior_predictives[posterior_predictives.shape[0]-1-window:].mean(axis=0)\n",
      "print \"y\\t\\tP(y|C) [gibbs]\\tP(y|C) [actual]\\tP(y|params)\\t|gibbs-actual|\"\n",
      "for y, ((a, b), c) in zip(it.product([0,1],repeat=D), zip(zip(PH, actual_posterior_predictive), nonbayes_posterior_predictive)):\n",
      "    print \"\\t\".join(map(str, [y, a, b, c, math.fabs(a-b)]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "y\t\tP(y|C) [gibbs]\tP(y|C) [actual]\tP(y|params)\t|gibbs-actual|\n",
        "(0, 0, 0)\t0.107049901235\t0.110024691358\t0.0679880284267\t0.00297479012346\n",
        "(0, 0, 1)\t0.065853154321\t0.0707160493827\t0.0803405782175\t0.00486289506173\n",
        "(0, 1, 0)\t0.197630191358\t0.177382716049\t0.181045167516\t0.0202474753086\n",
        "(0, 1, 1)\t0.129733419753\t0.130765432099\t0.155237303581\t0.00103201234568\n",
        "(1, 0, 0)\t0.129526209877\t0.140345679012\t0.0480592442522\t0.0108194691358\n",
        "(1, 0, 1)\t0.0659596234568\t0.0566913580247\t0.0374534032716\t0.0092682654321\n",
        "(1, 1, 0)\t0.197093697531\t0.216691358025\t0.322329953129\t0.0195976604938\n",
        "(1, 1, 1)\t0.107153802469\t0.0973827160494\t0.107546321606\t0.00977108641975\n"
       ]
      }
     ],
     "prompt_number": 167
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice how our Gibbs estimate of the distribution does not quite match the non-Bayesian one, which makes sense. Since $N$ is small in our case, our prior distribution is still weighing in non-negligibly. Finally, let's take a look at the KL-divergence (w.r.t. the *Bayesian* distribution with ground truth clusters) as the number of iterations increases:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(np.arange(0, niters, skip) + 1, posterior_predictive_kls)\n",
      "plt.xlabel('Iterations')\n",
      "plt.ylabel('Posterior predictive KL-divergence')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 165,
       "text": [
        "<matplotlib.text.Text at 0x7f105ba510d0>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEPCAYAAAB7rQKTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f105876f110>"
       ]
      }
     ],
     "prompt_number": 165
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Note that this distribution seems to converge more slowly than the posterior in the previous section. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}