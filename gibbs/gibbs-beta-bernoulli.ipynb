{
 "metadata": {
  "name": "",
  "signature": "sha256:51d8433a5b99bdeae5a703fde03acaa415c40770afc9cc67bdfc94ebb1e29219"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Gibbs sampling for the beta-bernoulli mixture model"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Bayesian beta-bernoulli mixture model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given a fixed $K$ and $D$, the beta-bernoulli mixture model is a generative model describing $D$-dimensional binary vectors ($y_i \\in \\{0,1\\}^{D}$) drawn from a $K$-component mixture.\n",
      "\n",
      "We can describe the probabilistic model as:\n",
      "\\begin{align*}\n",
      "  \\pi|\\alpha &\\sim \\text{Dirichlet}(\\{ \\frac{\\alpha}{K} \\}_{i=1}^{K}) \\\\\n",
      "  c_i|\\pi &\\sim \\text{Discrete}(\\pi) \\\\\n",
      "  p_d^{k} | \\beta,\\gamma &\\sim \\text{Beta}(\\beta, \\gamma) \\\\\n",
      "  y_i^{d} | c_i{=}k,p_d^{k} &\\sim \\text{Bernoulli}(p_d^{k})\n",
      "\\end{align*}\n",
      "Let us clear up some of this notation. $\\pi$ is a $K$-dimensional vector living in the $(K-1)$-dimensional probability simplex. For each $k \\in [K]$, $\\{p_d^k\\}_{d=1}^{D} \\in [0,1]^{D}$. Given $N$ data points, the assignment vector $\\{c_i\\}_{i=1}^{N} \\in \\{0, ..., K-1\\}^{N}$. The hyperparameters of this model are $\\mathcal{H} = (\\alpha, \\beta, \\gamma)$.\n",
      "\n",
      "As we will see later on, this model has nice analytical properties for Gibbs sampling, since the beta distribution is a nice conjugate prior for the bernoulli distribution.\n",
      "\n",
      "The inference problem we will consider is the following. Given a dataset $\\mathcal{Y} = \\{y_i\\}_{i=1}^{N}$, we want to learn the posterior distribution on the assignment vector $\\mathcal{C} = \\{c_i\\}_{i=1}^{N}$. That is, we want to be able to estimate and draw samples from $p(\\mathcal{C} | \\mathcal{Y}; \\mathcal{H})$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Gibbs sampling for estimating the posterior"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This problem can be solved efficiently with [Gibbs sampling](http://en.wikipedia.org/wiki/Gibbs_sampling), which is another [Markov-chain monte carlo](http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) (MCMC) variant. Let the notation $\\mathcal{C}_{\\neg i} = \\{ c_j \\in \\mathcal{C} : j \\neq i \\}$. The Gibbs sampler assumes we can efficiently sample from the distribution $p(c_i | \\mathcal{C}_{\\neg i}, \\mathcal{Y}; \\mathcal{H})$. Every iteration of the sampling then works by\n",
      "sampling for each $i \\in [N]$,\n",
      "\\begin{align*}\n",
      "    c^{(t)}_{i} \\gets p(c_i | \\{ c_j \\in \\mathcal{C}^{(t)} : j < i \\}, \\{ c_j \\in \\mathcal{C}^{(t-1)} : j > i \\}, \\mathcal{Y}; \\mathcal{H})\n",
      "\\end{align*}\n",
      "\n",
      "Let the notation $\\mathcal{Y}^{k} = \\{ y_i \\in \\mathcal{Y} : c_i = k \\}$ and $\\mathcal{Y}^{k}_{\\neg i} = \\{ y_j \\in \\mathcal{Y} : c_j = k, j \\neq i \\}$.\n",
      "It turns out, for the beta-bernoulli model, we can indeed [derive](http://homepage.tudelft.nl/19j49/Publications_files/TR_1.pdf) that\n",
      "\\begin{align*}\n",
      "  p(c_i{=}k|\\mathcal{C}_{\\neg i}, \\mathcal{Y};\\mathcal{H}) \\propto\n",
      "    \\frac{ |\\mathcal{Y}^{k}_{\\neg i}| + \\frac{\\alpha}{K} }{ N - 1 + \\alpha } \\prod_{d=1}^{D} \\frac{(\\beta+\\sum_{y_k\\in \\mathcal{Y}^{k}} y_k^{d})^{y_i^d} (  \\gamma + |\\mathcal{Y}^{k}| - \\sum_{y_k\\in \\mathcal{Y}^{k}} y_k^{d})^{(1-y_i^{d})} }{ \\beta + \\gamma + |\\mathcal{Y}^{k}| }\n",
      "\\end{align*}\n",
      "We can construct $p(c_i{=}k|\\mathcal{C}_{\\neg i}, \\mathcal{Y};\\mathcal{H})$ exactly by then enumerating through all $K$ clusters and normalizing."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Implementation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as sp\n",
      "import scipy.misc\n",
      "\n",
      "def discrete_sample(pmf):\n",
      "    # XXX: does numpy have something to do this?\n",
      "    coin = np.random.random()\n",
      "    cdf = np.cumsum(pmf)\n",
      "    a = np.where(coin >= cdf)[0]\n",
      "    if not a.shape[0]:\n",
      "        return 0\n",
      "    return min(a[-1]+1, pmf.shape[0]-1)\n",
      "\n",
      "def gibbs_beta_bernoulli(Y, K, alpha, beta, gamma, niters):\n",
      "    N, D = Y.shape\n",
      "    alpha, beta, gamma = map(float, [alpha, beta, gamma])\n",
      "\n",
      "    # start with random assignment\n",
      "    assignments = np.random.randint(0, K, size=N)\n",
      "\n",
      "    # initialize the sufficient statistics (cluster sums) accordingly\n",
      "    sums = np.zeros((K, D)) \n",
      "    cnts = np.zeros(K)\n",
      "    for yi, ci in zip(Y, assignments):\n",
      "        sums[ci] += yi\n",
      "        cnts[ci] += 1\n",
      "\n",
      "    history = np.zeros((niters, N), dtype=np.int64) \n",
      "    for t in xrange(niters):\n",
      "        for i, (yi, ci) in enumerate(zip(Y, assignments)):\n",
      "            # remove from SS\n",
      "            sums[ci] -= yi\n",
      "            cnts[ci] -= 1\n",
      "\n",
      "            # build log P(c_i=k | c_{\\i}, Y)\n",
      "            def fn(k):\n",
      "                lg_term1 = np.log(cnts[k] + alpha/K)\n",
      "                lg_term2 = np.log(N - 1 + alpha)\n",
      "                lg_term3 = D*np.log(beta + gamma + cnts[k] + 1) \n",
      "                def fn1(tup):\n",
      "                    d, yid = tup \n",
      "                    if yid:\n",
      "                        return np.log(beta + sums[k, d] + 1)\n",
      "                    else:\n",
      "                        return np.log(gamma + cnts[k] - sums[k, d]) \n",
      "                lg_term4 = sum(map(fn1, enumerate(yi)))\n",
      "                return lg_term1 - lg_term2 - lg_term3 + lg_term4\n",
      "\n",
      "            lg_dist = np.array(map(fn, xrange(K)))\n",
      "            lg_dist -= sp.misc.logsumexp(lg_dist) # normalize\n",
      "            dist = np.exp(lg_dist)\n",
      "\n",
      "            # reassign\n",
      "            ci = discrete_sample(dist)\n",
      "            assignments[i] = ci\n",
      "            sums[ci] += yi\n",
      "            cnts[ci] += 1\n",
      "        history[t] = assignments\n",
      "\n",
      "    return history"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Testing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us first generate a small toy dataset (using the beta-bernoulli model as the generative process). Then we will test our Gibbs sampler on this small dataset."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alpha, beta, gamma = 1., 1., 2.\n",
      "K = 2\n",
      "D = 3\n",
      "N = 5\n",
      "\n",
      "pis = np.random.dirichlet(alpha/K*np.ones(K))\n",
      "cis = np.array([discrete_sample(pis) for _ in xrange(N)])\n",
      "aks = np.random.beta(beta, gamma, size=(K, D))\n",
      "\n",
      "def bernoulli(p):\n",
      "    return 1. if np.random.random() <= p else 0.\n",
      "\n",
      "Y = np.zeros((N, D))\n",
      "for i in xrange(N):\n",
      "    Y[i] = np.array([bernoulli(aks[cis[i], d]) for d in xrange(D)])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "niters = 50000\n",
      "chain = gibbs_beta_bernoulli(Y, K, alpha, beta, gamma, niters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the natural question is, how do we verify that the Gibbs sampler is indeed drawing samples from $p(\\mathcal{C} | \\mathcal{Y})$. This is a bit trickier in the Bayesian case than the non-Bayesian case, since what we are really after is a *distribution* instead of, e.g. a point estimate.\n",
      "\n",
      "Luckily, for small problems (small $K$ and $N$), we can actually calculate $p(\\mathcal{C} | \\mathcal{Y}; \\mathcal{H})$ exactly by brute force enumeration. This is because $p(\\mathcal{C} | \\mathcal{Y} ; \\mathcal{H})$ is a discrete distribution of size $K^{N}$, and we can analytically calculate the joint distribution $p(\\mathcal{C}, \\mathcal{Y}; \\mathcal{H})$; from the joint, the posterior follows by $p(\\mathcal{C} | \\mathcal{Y} ) = \\frac{p(\\mathcal{C}, \\mathcal{Y})}{\\sum_{c} p(c ,\\mathcal{Y})}$, where the summation in the denominator is over all possible $K^N$ assignment vectors (from here on, we drop the hyperparameter dependence to ease notation).\n",
      "\n",
      "For an arbitrary $\\mathcal{C}, \\mathcal{Y}$, we have $p(\\mathcal{C}, \\mathcal{Y}) = p(\\mathcal{C}) p(\\mathcal{Y} | \\mathcal{C})$. From [page 2](http://homepage.tudelft.nl/19j49/Publications_files/TR_1.pdf), we have \n",
      "\\begin{align*}\n",
      "  p(\\mathcal{C}) = \\frac{\\Gamma(\\alpha)}{\\Gamma(|\\mathcal{Y}|+\\alpha)}\n",
      "      \\prod_{k=1}^{K} \\frac{\\Gamma( |\\mathcal{Y}^{k}| + \\frac{\\alpha}{K})}{ \\Gamma(\\frac{\\alpha}{K})}\n",
      "\\end{align*}\n",
      "where $\\Gamma(\\cdot)$ is the [Gamma](http://en.wikipedia.org/wiki/Gamma_function) function.\n",
      "We can derive $p(\\mathcal{Y}|\\mathcal{C})$ as follows.\n",
      "\\begin{align*}\n",
      "  p(\\mathcal{Y} | \\mathcal{C}) &= \n",
      "    \\prod_{k=1}^{K} p(\\mathcal{Y}^{k} | \\mathcal{C}^{k}) \\\\\n",
      "    &= \\prod_{k=1}^{K} \\int_{\\Theta_k} [\\prod_{y_i \\in \\mathcal{Y}^k} p(y_i | \\Theta_k) ] p(\\Theta_k ; \\mathcal{H}) \\; d\\Theta_k \n",
      "\\end{align*}\n",
      "Now for each $k$, we evaluate the inner integral as:\n",
      "\\begin{align*}\n",
      "  \\int_{\\theta_1,...,\\theta_D} \\prod_{d=1}^{D} \\left(\\prod_{y_i \\in \\mathcal{Y}^k} \\theta_d^{y_i^d} (1-\\theta_d)^{1-y_i^d}\\right) \\frac{1}{B(\\beta,\\gamma)} \\theta_d^{\\beta-1}(1-\\theta_d)^{\\gamma-1} \\; d\\theta_1...\\theta_{D}\n",
      "\\end{align*}\n",
      "where $B(\\beta,\\gamma)$ is the [Beta function](http://en.wikipedia.org/wiki/Beta_function). Noting that $\\int_{0}^{1} x^{(m-1)} (1-x)^{(n-1)} \\; dx = \\frac{\\Gamma(m)\\Gamma(n)}{\\Gamma(m+n)}$, we can simplify the above integral to:\n",
      "\\begin{align*}\n",
      "\\frac{1}{B(\\beta,\\gamma)^{D}} \\prod_{d=1}^{D} \\frac{\\Gamma( \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\beta) \\Gamma( |\\mathcal{Y}^{k}| - \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\gamma)}{ \\Gamma( |\\mathcal{Y}^{k}| + \\beta + \\gamma )}\n",
      "\\end{align*}\n",
      "And therefore,\n",
      "\\begin{align*}\n",
      "  p(\\mathcal{Y} | \\mathcal{C}) = \\frac{1}{B(\\beta,\\gamma)^{KD}} \\prod_{k=1}^{K} \\prod_{d=1}^{D} \\frac{\\Gamma( \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\beta) \\Gamma( |\\mathcal{Y}^{k}| - \\sum_{y_i \\in \\mathcal{Y}^k} y_i^d + \\gamma)}{ \\Gamma( |\\mathcal{Y}^{k}| + \\beta + \\gamma )}\n",
      "\\end{align*}\n",
      "\n",
      "Therefore, we can compute the posterior distribution of the data as follows:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy as sp\n",
      "import scipy.special\n",
      "import itertools as it\n",
      "import math\n",
      "\n",
      "def lg_pr_joint(C, Y, K, alpha, beta, gamma):\n",
      "    N, D = Y.shape\n",
      "    nks = np.bincount(C, minlength=K)\n",
      "    assert nks.shape[0] == K\n",
      "    assert C.shape[0] == N\n",
      "\n",
      "    # log P(C)\n",
      "    gammaln = sp.special.gammaln\n",
      "    betaln = sp.special.betaln\n",
      "    term1 = gammaln(alpha) - gammaln(N + alpha) - K*gammaln(alpha/K)\n",
      "    term2 = sum(gammaln(nk + alpha/K) for nk in nks)\n",
      "    lg_pC = term1 + term2\n",
      "\n",
      "    # log P(Y|C)\n",
      "    term1 = K*D*betaln(beta, gamma)\n",
      "    term2 = D*sum(gammaln(nk + beta + gamma) for nk in nks)\n",
      "    sums = np.zeros((K, D))\n",
      "    for yi, ci in zip(Y, C):\n",
      "        sums[ci] += yi\n",
      "    def fn1(nk, sum_yid):\n",
      "        assert nk >= sum_yid\n",
      "        return gammaln(sum_yid + beta) + gammaln(nk - sum_yid + gamma)\n",
      "    term3 = sum(sum(fn1(nk, yid) for yid in row) for nk, row in zip(nks, sums))\n",
      "    lg_pYgC = -term1 - term2 + term3\n",
      "    \n",
      "    return lg_pC + lg_pYgC\n",
      "\n",
      "def brute_force_posterior(Y, K, alpha, beta, gamma):\n",
      "    N, _ = Y.shape\n",
      "\n",
      "    # enumerate K^N cluster assignments\n",
      "    lg_pis = np.array([lg_pr_joint(np.array(C), Y, K, alpha, beta, gamma) for C in it.product(range(K), repeat=N)])\n",
      "    lg_pis -= sp.misc.logsumexp(lg_pis)\n",
      "    \n",
      "    return np.exp(lg_pis)\n",
      "\n",
      "actual_posterior = brute_force_posterior(Y, K, alpha, beta, gamma)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To measure the distance between the actual posterior distribution and that produced by our Gibbs sampler, we compare the KL-divergence of the actual and the empirical. Recall for discrete distributions the KL-divergence (relative entropy) is defined as\n",
      "\\begin{align*}\n",
      "  D(P||Q) = \\sum_{x} P(x) \\log\\frac{P(x)}{Q(x)}\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "smoothing = 1e-5\n",
      "skip = 100\n",
      "\n",
      "def kl(a, b):\n",
      "    return np.sum([p*np.log(p/q) for p, q in zip(a, b)])\n",
      "\n",
      "def histify(history, K):\n",
      "    _, N = history.shape\n",
      "    # generate an ID for each K^N element\n",
      "    idmap = { C : i for i, C in enumerate(it.product(range(K), repeat=N)) }\n",
      "    hist = np.zeros(K**N, dtype=np.float)\n",
      "    for h in history:\n",
      "        hist[idmap[tuple(h)]] += 1.0\n",
      "    return hist\n",
      "\n",
      "def fn(i):\n",
      "    hist = histify(chain[:i:skip], K) + smoothing\n",
      "    hist /= hist.sum()\n",
      "    return kl(actual_posterior, hist)\n",
      "\n",
      "iters = range(0, niters, skip)[1:]\n",
      "kls = map(fn, iters)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pylab as plt\n",
      "\n",
      "plt.plot(iters, kls)\n",
      "plt.xlabel('Iterations')\n",
      "plt.ylabel('KL-divergence')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "<matplotlib.text.Text at 0x7f105ec47690>"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEPCAYAAABCyrPIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHtnIAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x7f105ee92190>"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now to look at the posterior predictive distributions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def posterior_predictive(C, Y, K, alpha, beta, gamma):\n",
      "    N, D = Y.shape\n",
      "    assert C.shape[0] == N\n",
      "\n",
      "    nks = np.bincount(C, minlength=K)\n",
      "    sums = np.zeros((K, D)) \n",
      "    for yi, ci in zip(Y, C): \n",
      "        sums[ci] += yi\n",
      "\n",
      "    def fn(yvalue):\n",
      "        def fn1(nk, sum_yid, yd):\n",
      "            assert nk >= sum_yid\n",
      "            theta = (alpha + sum_yid) / (alpha + beta + nk) \n",
      "            assert theta >= 0.0 and theta <= 1.0 \n",
      "            return np.log(theta) if yd else np.log(1.-theta)\n",
      "        def fn2(nk, row):\n",
      "            assert len(yvalue) == row.shape[0]\n",
      "            term1 = np.log(nk + alpha/K) - np.log(N + alpha)\n",
      "            term2 = sum(fn1(nk, sum_yid, yd) for sum_yid, yd in zip(row, yvalue))\n",
      "            return term1 + term2\n",
      "        return sp.misc.logsumexp([fn2(nk, row) for nk, row in zip(nks, sums)])\n",
      "\n",
      "    yvalues = it.product([0, 1], repeat=D)\n",
      "    lg_pr_yvalue = map(fn, yvalues)\n",
      "    return np.exp(lg_pr_yvalue)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "actual_posterior_predictive = posterior_predictive(cis, Y, K, alpha, beta, gamma)\n",
      "posterior_predictives = np.array([\n",
      "    posterior_predictive(assignment, Y, K, alpha, beta, gamma) for assignment in chain[::skip]])\n",
      "    \n",
      "def fn1(i):\n",
      "    posteriors = posterior_predictives[:(i+1)].mean(axis=0)\n",
      "    return kl(actual_posterior_predictive, posteriors)\n",
      "\n",
      "posterior_predictive_kls = map(fn1, xrange(posterior_predictives.shape[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "Cannot cast array data from dtype('float64') to dtype('int64') according to the rule 'safe'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-10-e1d3b99f4b96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mactual_posterior_predictive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mposterior_predictive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m posterior_predictives = np.array([\n\u001b[1;32m----> 3\u001b[1;33m     posterior_predictive(assignment, Y, K, alpha, beta, gamma) for assignment in chain[::skip]])\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-8-d2aebccf763c>\u001b[0m in \u001b[0;36mposterior_predictive\u001b[1;34m(C, Y, K, alpha, beta, gamma)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mnks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0msums\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0myi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mci\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mTypeError\u001b[0m: Cannot cast array data from dtype('float64') to dtype('int64') according to the rule 'safe'"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(range(0, niters, skip)[1:], posterior_predictive_kls[1:])\n",
      "plt.xlabel('Iterations')\n",
      "plt.ylabel('Posterior predictive KL-divergence')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}